{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1fc1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859e89e9",
   "metadata": {},
   "source": [
    "**KNN for regression and classification**\n",
    "\n",
    "Previously during class, we learned about the KNN (K-Nearest Neighbour) algorithm - and how it can be used for both regression and classification.\n",
    "\n",
    "To start using it with scikit-learn, it's as easy as any other model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ae4885",
   "metadata": {},
   "source": [
    "*Regression*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a81364",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "model = KNeighborsRegressor(n_neighbors=3)                # remember, the number of neighbours is a hyperparameter that you'll have to tune\n",
    "\n",
    "model.fit(X_train,y_train)                                # train your model on the train set\n",
    "\n",
    "model.predict(X_test)                                     # predict on test set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f84492",
   "metadata": {},
   "source": [
    "*Classification*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f9b915f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "model = KNeighborsClassifier(n_neighbors=3)    \n",
    "\n",
    "model.fit(X_train,y_train)                               \n",
    "\n",
    "model.predict(X_test)                                     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04dc3c8",
   "metadata": {},
   "source": [
    "**Using KNN to impute missing values in our dataset**\n",
    "\n",
    "Apart from classification and regression, there is another very popular use-case for KNN - and that is to impute missing values of features\n",
    "in our dataset. \n",
    "\n",
    "The function is called KNNImputer, and you can read the documentation for it [here](https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html).\n",
    "\n",
    "Let's see how this works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c187a574",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset for marks of a student\n",
    "\n",
    "dict = {'Maths':[78, 90, 83, 95], \n",
    "        'Chemistry': [60, 79, 80, 82], \n",
    "        'Physics':[66, 71, 80, 78],\n",
    "        'Biology' : [78,83,75,np.nan],\n",
    "        'y' : ['y_1', 'y_2', 'y_3', 'y_4']}\n",
    "\n",
    "df = pd.DataFrame(dict)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e02cec2",
   "metadata": {},
   "source": [
    "Let's pretend that Maths, Chemistry, Physics and Biology are the features and the column y, the target. \n",
    "\n",
    "The target can be anything (both discrete and continous), it doesn't matter here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a68d515",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.drop(columns=['y']), df['y']\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231d8159",
   "metadata": {},
   "source": [
    "Wee see that we have a problem in our biology column, it's missing a value. What should we replace it with?\n",
    "\n",
    "Well, one very simple and good thing to try is to use the **other columns** as features, \n",
    "and in doing so, find the nereast neighbours to our row (3) that is missing a value in Biology.\n",
    "\n",
    "Once we've found our nearest neighbours - we'll impute our missing value with the average of *their* values for Biology."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62283dd3",
   "metadata": {},
   "source": [
    "Scikit-learn has a neat function that does precisely this for us automatically:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045737e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=2)          # find the two nearest neighbours\n",
    "\n",
    "X_after_imputation = imputer.fit_transform(X)\n",
    "\n",
    "X_after_imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d79fa43f",
   "metadata": {},
   "source": [
    "As we can see above, the algorithm seem to have identified row 1 and 2 as the nearest neighbours, and imputed our missing value with their average\n",
    "values for Biology."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54c5ee2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(83+75)/2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78944ecb",
   "metadata": {},
   "source": [
    "**Imputing several missing values**\n",
    "\n",
    "KNN imputer can actually handle imputing several missing values, simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b49be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset for marks of a student\n",
    "\n",
    "dict = {'Maths':[78, 90, 83, 95], \n",
    "        'Chemistry': [60, 79, 80, 82], \n",
    "        'Physics':[66, 71, 80, 78],\n",
    "        'Biology' : [78,83,np.nan,np.nan],\n",
    "        'y' : ['y_1', 'y_2', 'y_3', 'y_4']}\n",
    "\n",
    "df = pd.DataFrame(dict)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69803b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.drop(columns=['y']), df['y']\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01dc1d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = KNNImputer(n_neighbors=2)          # find the two nearest neighbours\n",
    "\n",
    "X_after_imputation = imputer.fit_transform(X)\n",
    "\n",
    "X_after_imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a268734",
   "metadata": {},
   "source": [
    "**It can even handle missing values in multiple columns aswell**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa28582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataset for marks of a student\n",
    "\n",
    "dict = {'Maths':[np.nan, 90, 83, 95], \n",
    "        'Chemistry': [60, np.nan, 80, 82], \n",
    "        'Physics':[66, 71, 80, 78],\n",
    "        'Biology' : [78,83,np.nan,np.nan],\n",
    "        'y' : ['y_1', 'y_2', 'y_3', 'y_4']}\n",
    "\n",
    "df = pd.DataFrame(dict)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b779e02a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.drop(columns=['y']), df['y']\n",
    "\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9449da",
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = KNNImputer(n_neighbors=2)          # find the two nearest neighbours\n",
    "\n",
    "X_after_imputation = imputer.fit_transform(X)\n",
    "\n",
    "X_after_imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8534b08f",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d6a919",
   "metadata": {},
   "source": [
    "## Some caveats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a970cc",
   "metadata": {},
   "source": [
    "1. Always be mindful when imputing missing values, don't just use KNNImputer mindlessly and hope for the best.\n",
    "2. When using KNNImputer to fill out missing values, we don't actually need to scale our features - since it doesn't affect performance significantly.\n",
    "3. However, as mentioned previously, when using KNN for regression or classification - we must scale our features before!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
