{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b511518e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a58a863",
   "metadata": {},
   "source": [
    "##### Inom AI-utveckling handlar allt i grund och botten om att hitta minsta värden för vissa funktioner. För det ändamålet gick vi under senaste lektionen igenom den metod som används, för i princip all moden AI-utveckling, för att göra detta: Gradient Descent.\n",
    "\n",
    "#### Syftet med följande uppgifter är att vi ska få en lite bättre känsla för hur Gradient Descent fungerar"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fdf32a",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31338bbe",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afe2b543",
   "metadata": {},
   "source": [
    "Vi definierar först här en funktion som vi vill hitta minimum till. Specifikt vill vi hitta det x-värde som ger funktionens minsta värde. Vi definierar även funktionens derivata - den behövs för Gradient Descent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4eb943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    " \n",
    "    return x**2-4*x+4\n",
    "\n",
    "def f_derivative(x):\n",
    "    \n",
    "    return 2*x-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15ae7e58",
   "metadata": {},
   "source": [
    "Vi definierar nu funktionen som Utför Gradient Descent, vilket interativt hittar oss fram till det x-värde som ger oss minsta funktionsvärdet.\n",
    "\n",
    "OBS. Du behöver **INTE** gå igenom för att försöka förstå den här koden i detalj. Det är bra om du försöker, men det absolut inget inget krav.\n",
    "\n",
    "Det enda vi behöver göra är att försöka förstå kodens output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a401579",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(f, f_derivative, learning_rate=0.1, delta = 0.001):\n",
    "    \n",
    "    # learning rate anger är \"hur stora steg\" vi backar i gradientens riktning\n",
    "    # delta kontrollar när loopen stannar (vi stannar då funktionsminskningen är mindre än delta)\n",
    "    \n",
    "    x_values = []                         # lista för att spara alla våra x-värden\n",
    "    function_values = []                  # lista för att spara alla funktions-värden\n",
    "    \n",
    "    x = int(input('Ange heltalsgissning på x: '))     #här får användaren gissa ett värde på x, för vilket minimum på funktionen ges\n",
    "    function_value = f(x)                              #beräkna funktionens värde vid gissningen på x\n",
    "    \n",
    "    x_values.append(x)                                 #lägg till vårt första x-värde\n",
    "    function_values.append(function_value)             #lägg till vårt första funktions-värde\n",
    "    \n",
    "    iteration = 0                         #variabel som anger vilken iteration vi är i\n",
    "    \n",
    "    #----------------------------------------------------------------------\n",
    "    #den här delen har ingenting med agoritmen att göra. vi genererar här endast lite siffror att kunna använda i våra plots\n",
    "    \n",
    "    distance = int(np.abs(2-x))\n",
    "    \n",
    "    x_values_to_plot = [x_value for x_value in np.linspace(2-distance-0.5,2+distance+0.5, 100)]\n",
    "    f_values_to_plot = [f(x) for x in x_values_to_plot]\n",
    "    \n",
    "    plt.plot(x_values_to_plot, f_values_to_plot, label='f(x)', color='blue')\n",
    "    plt.scatter(x_values, function_values, facecolors='none', edgecolors='r')\n",
    "    plt.ylim(0,)\n",
    "    plt.legend()\n",
    "    plt.xlabel('x')\n",
    "    plt.show()\n",
    "    \n",
    "    print(f'iteration: {iteration}', end='\\n\\n')\n",
    "\n",
    "    print(f'Current value of x: {x_values[-1]}')\n",
    "    print(f'Current value of f: {function_values[-1]}', end='\\n\\n')\n",
    "    #----------------------------------------------------------------------\n",
    "    \n",
    "    new_function_value = 0              #initiera en ny variabel som vi ska använda för att sätta nya funktions värden i\n",
    "    \n",
    "    while True:  #loopa här tills vi bryter själva genom en sats i loopen\n",
    "        \n",
    "        iteration+=1\n",
    "        \n",
    "        derivative_value = f_derivative(x)      #beräkna derivatans värde för nuvarande värde på x\n",
    "        \n",
    "        x = x - learning_rate*derivative_value  #uppdatera x till ett nytt värde, genom att backa lite i gradientens riktning\n",
    "        new_function_value = f(x)               #beräkna funktionens värde för det nya värdet på x\n",
    "        \n",
    "        #if np.abs(function_value-new_function_value) < delta: #bryt här om funktions värde för det nya värdet på x, inte minskar särskilt mycket\n",
    "        if np.abs(new_function_value) < delta:\n",
    "            \n",
    "            break\n",
    "        \n",
    "        else:\n",
    "                \n",
    "            function_value = new_function_value #sätt det nya värdet på funktionen som vårt ordinarie värde i variabeln function_value \n",
    "            \n",
    "            x_values.append(np.round(x,2))                       #lägg till vårt nya x-värde\n",
    "            function_values.append(function_value)               #Lägg till vårt nya funktionsvärde\n",
    "            \n",
    "            plt.plot(x_values_to_plot, f_values_to_plot, label='f(x)', color='blue')\n",
    "            plt.scatter(x_values, function_values, facecolors='none', edgecolors='r')\n",
    "            plt.ylim(0,)\n",
    "            plt.legend()\n",
    "            plt.show()\n",
    "            \n",
    "            print(f'iteration: {iteration}', end='\\n\\n')\n",
    "            \n",
    "            print(f'Current value of x: {x_values[-1]}')\n",
    "            print(f'Current value of f: {np.round(function_values[-1],4)}', end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d4a1dc1",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4135c2e5",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251446dc",
   "metadata": {},
   "source": [
    "**UPPGIFTER**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33011562",
   "metadata": {},
   "source": [
    "**1)**\n",
    "\n",
    "Aktivera funktionen och testa den för olika värden på startgissningen. Testa både negativa och positiva värden. Vad händer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597fbe64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gradient_descent(f, f_derivative)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15592946",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbf960f",
   "metadata": {},
   "source": [
    "**2)**\n",
    "\n",
    "Vi kan ge funktionen ytterligare en input, learning rate. I klassen gick vi igenom att learning rate är \"hur mycket\" vi backar i gradientens riktning (dvs, hur snabbt vi närmare oss sanna värdet av x) vid varje iteration. \n",
    "\n",
    "Välj x=6 som startgissning, och testa därefter att köra igenom funktionen för följande värden på learning rate: \n",
    "\n",
    "[0.1, 0.01, 0,001]\n",
    "\n",
    "Vad händer? Hur många iterationen krävs i varje fall för att slutföra funktionen?\n",
    "\n",
    "Tips: du kan manuellt avbryta funktionen om det tar för lång tid..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4e6ffa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gradient_descent(f, f_derivative, learning_rate = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3fd7e1",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb436c1d",
   "metadata": {},
   "source": [
    "**3*)**\n",
    "\n",
    "\n",
    "Vi har i uppgiften ovan testat vad som händer om man har relativt små värden på learning rate. Nu ska vi testa motsatsen, dvs lite större värden på learning rate.\n",
    "\n",
    "Kör funktionen igen, men x=6 som startgissning, men denna gång testa följande learning rates:\n",
    "\n",
    "[0.8, 1, 1.2]\n",
    "\n",
    "Vad händer? \n",
    "\n",
    "Återigen, du kan manuellt abryta funktionen om det tar för lång tid..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7109634",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gradient_descent(f, f_derivative, learning_rate = 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3153398",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
