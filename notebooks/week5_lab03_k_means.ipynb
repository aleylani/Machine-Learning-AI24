{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1fc1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859e89e9",
   "metadata": {},
   "source": [
    "**Introduction to Clustering**\n",
    "\n",
    "Clustering is a technique used in unsupervised learning, where the goal is to group similar data points together based on certain features or characteristics. Unlike supervised learning, clustering does not have labeled data, and the algorithm needs to find the inherent structure within the data itself.\n",
    "\n",
    "\n",
    "What is *K-Means clustering*?\n",
    "\n",
    "K-Means clustering is one of the simplest and most popular clustering algorithms. It partitions the data into a predefined number of clusters, where each data point belongs to the cluster with the nearest mean (centroid). The algorithm iteratively assigns data points to the nearest centroid and then recalculates the centroids based on the mean of the data points in each cluster. This process continues until the centroids no longer change significantly or a specified number of iterations is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04dc3c8",
   "metadata": {},
   "source": [
    "**Let's plot some synthetic data to work with**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbac8fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 300         # Total number of data points\n",
    "centers = 6             # Number of clusters in the data\n",
    "random_state = 104      # Random state for reproducibility\n",
    "\n",
    "# Create synthetic data using make_blobs\n",
    "X, y = make_blobs(n_samples=n_samples, centers=centers, random_state=random_state)\n",
    "\n",
    "# Visualizing the generated data\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.title('Synthetic Data Generated by make_blobs')\n",
    "plt.xlabel('Feature 1')\n",
    "plt.ylabel('Feature 2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c03da4",
   "metadata": {},
   "source": [
    "**Implementing K-Mean clustering from scratch**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c2ca651",
   "metadata": {},
   "source": [
    "In order to really understand what's going on under the hood of the algorithm, we'll implement K-Means directly through Python. \n",
    "\n",
    "Later, we'll see that the whole process below will be handled by the KMeans-function in scikit-learn.\n",
    "\n",
    "*In short:*\n",
    "\n",
    "The K-Means algorithm works by iteratively assigning data points to the nearest cluster centroid and then updating the centroids based \n",
    "on the mean of the points assigned to each cluster. This process continues until convergence, where centroids no longer change significantly \n",
    "or a predefined number of iterations is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2caea381",
   "metadata": {},
   "source": [
    "*Define the necessary functions:*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e345ee38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_centroids(X, k):\n",
    "    indices = np.random.choice(len(X), k, replace=False)\n",
    "    centroids = X[indices]\n",
    "    return centroids\n",
    "\n",
    "def assign_clusters(X, centroids):\n",
    "    distances = np.sqrt(((X - centroids[:, np.newaxis])**2).sum(axis=2))\n",
    "    clusters = np.argmin(distances, axis=0)\n",
    "    return clusters\n",
    "\n",
    "def update_centroids(X, clusters, k):\n",
    "    centroids = np.zeros((k, X.shape[1]))\n",
    "    for i in range(k):\n",
    "        centroids[i] = X[clusters == i].mean(axis=0)\n",
    "    return centroids\n",
    "\n",
    "def kmeans_from_scratch(X, k, max_iters=100):\n",
    "    centroids = initialize_centroids(X, k)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(X[:, 0], X[:, 1], s=50)\n",
    "    plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=100, marker='X')\n",
    "    plt.xlabel('Feature 1')\n",
    "    plt.ylabel('Feature 2')\n",
    "    plt.title('Step 1: Random Initialization of Centroids')\n",
    "    plt.show()\n",
    "    iterations = 0\n",
    "    for i in range(max_iters):\n",
    "        clusters = assign_clusters(X, centroids)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.scatter(X[:, 0], X[:, 1], c=clusters, cmap='viridis', s=50, alpha=0.5)\n",
    "        plt.scatter(centroids[:, 0], centroids[:, 1], c='red', s=100, marker='X')\n",
    "        plt.xlabel('Feature 1')\n",
    "        plt.ylabel('Feature 2')\n",
    "        plt.title(f'Step {i+2+iterations}: Assign Points to Nearest Centroids')\n",
    "        plt.show()\n",
    "        new_centroids = update_centroids(X, clusters, k)\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.scatter(X[:, 0], X[:, 1], s=50)\n",
    "        plt.scatter(new_centroids[:, 0], new_centroids[:, 1], c='red', s=100, marker='X')\n",
    "        plt.xlabel('Feature 1')\n",
    "        plt.ylabel('Feature 2')\n",
    "        plt.title(f'Step {i+3+iterations}: Update Centroids')\n",
    "        plt.show()\n",
    "        iterations += 1\n",
    "        if np.all(centroids == new_centroids):\n",
    "            break\n",
    "        centroids = new_centroids\n",
    "    return centroids, clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c19a98",
   "metadata": {},
   "source": [
    "Great, now we proceed with the implementations. Before we start, you need to decide on\n",
    "the number of cluster *k* to find"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c89aa5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying K-Means from scratch\n",
    "\n",
    "\n",
    "k = 6                      # decide on the the number of clusters the algorithm should look out for\n",
    "\n",
    "# initialize the algorithm\n",
    "final_centroids, final_clusters = kmeans_from_scratch(X, k)\n",
    "\n",
    "# print coordinates for the identified clusters\n",
    "print(\"Final centroids:\")\n",
    "print(final_centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a75b10b",
   "metadata": {},
   "source": [
    "**scikit-learn implementation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd47620",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=k)           # remember that we defined the number k previousy\n",
    "\n",
    "kmeans.fit(X)                           # use KMeans to find k clusters in the data\n",
    "\n",
    "# We can now extract the identified centroids\n",
    "centroids = kmeans.cluster_centers_\n",
    "\n",
    "for i, centroid in enumerate(centroids):\n",
    "    print(f'Coordinates for cluster {i}: {centroid}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94fbbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans.inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13792ff1",
   "metadata": {},
   "source": [
    "Compare the obtained centroids using scikit-learns k-means, with ours!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b60f262",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da99cc98",
   "metadata": {},
   "source": [
    "## Challenges \n",
    "\n",
    "**Task 0**\n",
    "\n",
    "Understand the following points:\n",
    "\n",
    "\n",
    "Note that, like KNN, KMeans is a distance based algorithm and it's thus *critical* to make sure that our features are at the same scale. Most often, you therefore have to do feature scaling to get sensible results from Kmeans.\n",
    "\n",
    "Also, it goes without saying perhaps: you can use more than two features. In fact, you can use as many as you like.\n",
    "\n",
    "**Task 1**\n",
    "\n",
    "Try changing the random state for the synthetic data generator - so that you get different data from what we already have.\n",
    "\n",
    "Then, re-run the simulation and check the results for both kmean_from_scratch and kmeans from scikit-learn.\n",
    "\n",
    "*NOTE*: \n",
    "\n",
    "Sometimes, if you're unlucky, you'll see that KMeans wont actually converge and instead get stuck, yielding a bad end result.\n",
    "\n",
    "This happens when you sometimes have a bad/unlucky initialization of your clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7cfb35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cda6f27c",
   "metadata": {},
   "source": [
    "**Task 2**\n",
    "\n",
    "Play around some bit more with KMeans. This time, try also changing the number of clusters when generating the data. Also try changing the number of clusters *k* that your algorithm should try to find. \n",
    "\n",
    "*NOTE*:\n",
    "\n",
    "The number of clusters in the data, and the number of clusters for the algorithm to find do **NOT** have to be the same. Try having different values for each and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f2bfb64",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3345a0b9",
   "metadata": {},
   "source": [
    "**Task 3**\n",
    "\n",
    "How do we choose the optimal value of *k* for KMeans? We very rarely know in advance the amount of clusters to identify in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19dc9d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "cost =[]\n",
    "\n",
    "\n",
    "for k in range(1, 15):\n",
    "    \n",
    "    KM = KMeans(n_clusters = k, max_iter = 500)\n",
    "    KM.fit(X)\n",
    "     \n",
    "    # calculates squared error\n",
    "    # for the clustered points\n",
    "    cost.append(KM.inertia_)     \n",
    " \n",
    "# plot the cost against K values\n",
    "plt.plot(range(1, 15), cost, color ='g', linewidth ='3')\n",
    "plt.xlabel(\"Value of K\")\n",
    "plt.ylabel(\"Squared Error (Cost)\")\n",
    "plt.show() # clear the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8eabd7",
   "metadata": {},
   "source": [
    "This is called the Elbow-method and you can read more about it [here](https://www.geeksforgeeks.org/ml-determine-the-optimal-value-of-k-in-k-means-clustering/)!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55238f7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
