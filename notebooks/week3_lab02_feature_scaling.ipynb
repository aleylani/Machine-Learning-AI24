{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1fc1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131cc8ba",
   "metadata": {},
   "source": [
    "## Feature scaling\n",
    "\n",
    "In Machine Learning, oftentimes it's very important to *scale* our feature data before training on it. \n",
    "\n",
    "To understand why, assume we have a dataset with the following features\n",
    "\n",
    "**age** (18-100 years), \n",
    "\n",
    "**salary** (25.000-75.000 euros) \n",
    "\n",
    "**height** (1-2 meters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846489ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# Generate 10 rows of random data using list comprehension\n",
    "random_data = [{'age': random.randint(18, 100),\n",
    "                'salary': random.randint(25000, 75000),\n",
    "                'height': round(random.uniform(1, 2), 2), \n",
    "                'y': random.uniform(0,100)} \n",
    "                for _ in range(10)]\n",
    "\n",
    "df = pd.DataFrame(random_data)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2304eaee",
   "metadata": {},
   "source": [
    "If we try to use these features into (e.g.,) a linear model that predicts a target *y*, we will have the following\n",
    "\n",
    "$$ y = w_3 \\cdot \\text(age) + w_2 \\cdot \\text(salary) + w_1 \\cdot \\text(height) + w_0 $$\n",
    "\n",
    "We can notice a *risk* here: \n",
    "\n",
    "The contribution to *y* from of the feature Salary might dominate the contribution from the other features, since it's can take on so much larger values than the rest. \n",
    "\n",
    "Even Age runs at risk at dominating height.\n",
    "\n",
    "Many models (not all!) are very sensitive to the scale difference between features. Indeed, the most advanced models (such as Neural Networks) are very sensitive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2ed956",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## The solution\n",
    "\n",
    "To bypass the above problem, we can *re-scale* our features so that they end up in the aprox. same range.\n",
    "\n",
    "**The good news**\n",
    "\n",
    "This does not affect the inherent relationship between relationship between our features and targets (what we're ultimately trying to modl)at all! \n",
    "\n",
    "Let's test this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ff9489",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = np.array([x for x in range(21)])\n",
    "y = np.array([3*x**2 for x in X])\n",
    "\n",
    "plt.scatter(X,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "764ecbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we scale x, by dividing by it's highest value\n",
    "\n",
    "X_scaled = X/max(X)\n",
    "\n",
    "plt.scatter(X_scaled, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26f8874",
   "metadata": {},
   "source": [
    "Notice that the relationship between our feature and target has not changed at all! Only the scale of the x-values has changed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668653c3",
   "metadata": {},
   "source": [
    "**Conclusion:**\n",
    "\n",
    "Feature scaling is required for many machine learning models, but not all model. However, performing feture scaling doesn't hurt performance for the models that don't require it!\n",
    "\n",
    "Therefore, performing feature scaling is *almost* always a good idea!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9231d439",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Types of Feature Scaling**\n",
    "\n",
    "The goal of feature scaling is to bring the values of the features close to the range [-1,1]. It's not super important exactly what range you scale to, we just want to get rid of the massive scale differences that we sometime have.\n",
    "\n",
    "There are several different types of feature scaling methods, one of which we used above.\n",
    "\n",
    "What we did there, since the feature x was strictly positive, is we simply divided it by it's highest value. Simple, and elegant. This scales the feature x down to the range [0,1]. So a strictly positive feature got scaled down to something else strictly positive. Nice.\n",
    "\n",
    "There are however also different methods. One of which is *Standardization*. This is done by performing this transformation on each value in the feature x.\n",
    "\n",
    "$$ x^{scaled} = \\frac{x^{unscaled} - \\bar{x}}{\\sigma_x}$$\n",
    "\n",
    "where $ \\bar{x}$ is the mean value of the feature x, and $\\sigma_x$ is the standard deviation of feature feature x.\n",
    "\n",
    "**På svenska:** Det här kommer transformera alla x-values i vår feature kolumn till 'antal standardavvikelser' from medelvärdet. Kommer ni ihåg från statistiken?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b43fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's us test standardizing a feature column\n",
    "\n",
    "X = np.array([random.randint(-20,20) for x in range(10)])\n",
    "y = np.array([3*x**2 for x in X])\n",
    "\n",
    "plt.scatter(X,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3feb4a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# reshape feature to 2D, since it's required here\n",
    "\n",
    "X = X.reshape(-1,1)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)                       # this line calculates the mean and the standard deviation of the feature\n",
    "\n",
    "print('Mean of x:', scaler.mean_)\n",
    "print('Std of x:', scaler.scale_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0df1397e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform our feature\n",
    "\n",
    "X_standardized = scaler.transform(X)\n",
    "\n",
    "plt.scatter(X_standardized,y)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e646505",
   "metadata": {},
   "source": [
    "Great, that was easy and we, again, see that the relationship between feature and target has not changed.\n",
    "\n",
    "I usually use standardscaler for features that can take both negative and positive values, while using the method of dividing by the max value for features with strictly positive values.\n",
    "\n",
    "Again, it's not superimportant. Bringing different features to aprox the same range is what's most important."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "051807fe",
   "metadata": {},
   "source": [
    "**BTW**\n",
    "\n",
    "Even for features that can assume negative values - you can actually simply just divide by the absolute highest value there too. For this course, this is enough. You don't really have to work with the Standardscaler above, if you don't want to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb205736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's us test standardizing a feature column\n",
    "\n",
    "X = np.array([random.randint(-20,20) for x in range(10)])\n",
    "y = np.array([3*x**2 for x in X])\n",
    "\n",
    "plt.scatter(X,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962616ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(max(X))\n",
    "\n",
    "print(min(X))\n",
    "\n",
    "absolute_highest_value = max(np.abs(min(X)), np.abs(max(X)))\n",
    "\n",
    "print(absolute_highest_value)\n",
    "\n",
    "X_scaled = X/absolute_highest_value\n",
    "\n",
    "plt.scatter(X_scaled,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c27bb6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Important Caveats\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c721238",
   "metadata": {},
   "source": [
    "\n",
    "    1. \n",
    "\n",
    "Only scale the features, not the targets! \n",
    "\n",
    "    2. \n",
    "    \n",
    "**Very important**\n",
    "\n",
    "Only ever use statistics from the Train set to perform scaling! Never ever use statistics from the test set. The reason is that this causes what call *data leakage* in which information from the test set (which is supposed to be completely unseen) leaks into the training data. This can give us very optimistic (but severly misleading) results when we try to interpret the performance on the test set.\n",
    "\n",
    "Here's how to do it correctly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352d4c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data, again\n",
    "\n",
    "X = np.array([x for x in range(-21, 21)])\n",
    "y = np.array([3*x**2 for x in X])\n",
    "\n",
    "plt.scatter(X,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31fd3df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "test_size = 0.2\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,\n",
    "                                                    y,\n",
    "                                                    test_size=test_size, \n",
    "                                                    random_state=42)\n",
    "\n",
    "\n",
    "X_train = X_train.reshape(-1,1)\n",
    "X_test = X_test.reshape(-1,1)\n",
    "\n",
    "y_train = y_train.reshape(-1,1)\n",
    "y_test = y_test.reshape(-1,1)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb1ff4d",
   "metadata": {},
   "source": [
    "Now we start the process of scaling our feature. \n",
    "\n",
    "**Note that we only calculate the mean and std on the train set**, but that we then apply it to both the train and test sets.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ee7c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)             # calculate mean and std of the train set\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)        # transform the train set using the statistic above\n",
    "X_test_scaled = scaler.transform(X_test)          # transform the test set using the same statistics above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec41ce1b",
   "metadata": {},
   "source": [
    "**Alternative 2**\n",
    "\n",
    "Dividing by absolutely max value, instead of using Standarscaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46a48de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "absolute_highest_value = max(np.abs(min(X_train)), np.abs(max(X_train)))\n",
    "\n",
    "print(absolute_highest_value)\n",
    "\n",
    "X_train_scaled = X_train/absolute_highest_value\n",
    "X_test_scaled = X_test/absolute_highest_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6acc8a8d",
   "metadata": {},
   "source": [
    "Great - we are now ready to train our model using this scaled feature!\n",
    "\n",
    "It's as simple as\n",
    "\n",
    "model.fit(X_train_scaled, y_train)\n",
    "\n",
    "and the\n",
    "\n",
    "model.predict(X_train_scaled)\n",
    "\n",
    "model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ede755",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "We have actually seen scaled features before, most recently in the bike demand data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "602a94fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('../data/bike_rental/hour.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bdad4d2",
   "metadata": {},
   "source": [
    "Have a look at e.g., the features temp and atemp. They are scaled. \n",
    "\n",
    "It even says so if you take a look at the [documentation.](https://archive.ics.uci.edu/dataset/275/bike+sharing+dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08455208",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dfe42b7",
   "metadata": {},
   "source": [
    "**Fantastic, now you know how to perform feature scaling! It's a good habit to do it.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916486f2",
   "metadata": {},
   "source": [
    "**Warning: do NOT scale categorical features (ask Sunny why)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b103d2e6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
