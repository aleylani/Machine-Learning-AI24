{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1fc1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea40cf3a",
   "metadata": {},
   "source": [
    "**Unsupervised Learning and PCA**\n",
    "\n",
    "In machine learning, as we've learned previously, unsupervised learning is a category of algorithms used to discover patterns and relationships within data without explicitly labeled outcomes (targets). Unlike supervised learning, where the algorithm learns from labeled data to predict outcomes, unsupervised learning deals with raw, unlabeled data and aims to extract meaningful insights or representations from it.\n",
    "\n",
    "Previously, we learned about KMeans, which is a form of unsupervised learning that's used for finding clusters withing your data.\n",
    "\n",
    "Principal Component Analysis (PCA) is another technique in unsupervised learning, but where the application is *dimensionality reduction* in data. We talke alot earlier about the importance of not having too many features in our data, since that could lead to the curse of dimensionality. In our previous labs, we actually worked alot with dimensionality reduction by iteratively removing features and re-training our models, in order to try to keep as few features as possible.\n",
    "\n",
    "PCA can automatically reduce dimensions in our data. It identifies the directions of maximum variance in high-dimensional data and projects it onto a lower-dimensional subspace while preserving the essential structure of the data. By reducing the number of features or dimensions, PCA can help in simplifying the data and improving computational efficiency while retaining most of the information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd20b11",
   "metadata": {},
   "source": [
    "**Objective**\n",
    "\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique commonly used in machine learning and data analysis. Its primary goal is to simplify the complexity of high-dimensional data by transforming it into a lower-dimensional space while retaining most of the relevant information. PCA achieves this by identifying the directions (or principal components) that capture the maximum variance in the data.\n",
    "\n",
    "In this context, we're once again working with the Iris dataset, a popular dataset in machine learning. It consists of measurements of various features of iris flowers, such as sepal length, sepal width, petal length, and petal width. Our goal is to visualize the data and apply PCA to reduce its dimensionality while preserving its underlying structure.\n",
    "\n",
    "*Note*: PCA is usually used on datasets with many dimensions(features) in order to reduce that amount of features into something much lower, but we choose to apply it on the iris dataset here so that we clearly can see what's going on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6003b63b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc183db",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.datasets import make_blobs\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "iris_df = pd.read_csv('../data/IRIS.csv')\n",
    "\n",
    "iris_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c488fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note, the iris dataset has 4 features and 1 target, but just for simplicity and visualization purposes, we'll drop one of the features instantly\n",
    "\n",
    "iris_df = iris_df[['sepal_length', 'petal_length', 'petal_width', 'species']]\n",
    "\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095ab802",
   "metadata": {},
   "source": [
    "Split the features from the targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d9b486",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = iris_df[['sepal_length', 'petal_length', 'petal_width']], iris_df['species']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4382f2",
   "metadata": {},
   "source": [
    "Plot the 3 features, and the target as the color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5889dbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "df = px.data.iris()\n",
    "fig = px.scatter_3d(iris_df, x='sepal_length', y='petal_length', z='petal_width', color='species')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81510214",
   "metadata": {},
   "source": [
    "Now we perform PCA on the **features** here, to find the 2 principle components (most important directions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "537c502c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA on the features\n",
    "# Note, we DONT use the targets here\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "iris_reduced_df = pd.DataFrame(X_pca)\n",
    "\n",
    "iris_reduced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7156a3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's also add the targets from earlier to our newly reduced data, in order to be able to plot\n",
    "iris_reduced_df['species'] = y\n",
    "\n",
    "iris_reduced_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cacbedc",
   "metadata": {},
   "source": [
    "As you can see, the dimensionality of our features have decreased from 3 to 2. \n",
    "\n",
    "**But**, which features are they?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be689e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the results with colors representing different classes\n",
    "fig = px.scatter(iris_reduced_df, x=0, y=1, color='species', title='PCA of Iris Dataset (2D)')\n",
    "fig.update_xaxes(title_text='Prinpical Component 1')\n",
    "fig.update_yaxes(title_text='Prinpical Component 2')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b50b020e",
   "metadata": {},
   "source": [
    "As you can see, we've reduced the feature dimensions from 3 to 2. PCA has mathematicall identified the 2 axis which account for the most variance in our original data, thus preserving as much information as possible.\n",
    "\n",
    "However, we have lost *some* information!\n",
    "\n",
    "**Importantly**, the 2 principle component axis we found are neither our original sepal_length, sepal_width or petal_width! Rather, it's a combination of all of them, mathematically found in a way that keeps as much information of the inherent structure of the data as possible!\n",
    "\n",
    "A downside of PCA is thus that it makes our newly found features *less* interpretable since we can't immediatly say what they actually mean."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab20626",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca92561e",
   "metadata": {},
   "source": [
    "We are free to choose the amount of principle components we'd like to use. Let's repreat the experiment above, but this time only keep the single most important dimension in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce9433b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d2413f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reduce the feature space into a single\n",
    "\n",
    "pca = PCA(n_components=1)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "iris_further_reduced_df = pd.DataFrame(X_pca)\n",
    "\n",
    "iris_further_reduced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e426ef6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_further_reduced_df['species'] = y\n",
    "\n",
    "iris_further_reduced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5465715",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 6))\n",
    "for species in iris_reduced_df['species'].unique():\n",
    "    plt.scatter(iris_reduced_df[iris_reduced_df['species'] == species][0], \n",
    "                [0] * len(iris_reduced_df[iris_reduced_df['species'] == species][0]), \n",
    "                label=species)\n",
    "\n",
    "plt.xlabel('PC1')\n",
    "plt.title('PCA of Iris Dataset (1D)')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "710770aa",
   "metadata": {},
   "source": [
    "We have now reduced our feature space into the single most important dimension in terms of the amount of varience of data. We have indeed lost information on the way, though - which is natural to do just as when you manually drop features during feature engineering.\n",
    "\n",
    "However, sometimes, as mentioned many times before, it is important to reduce the dimensionality of your feature space. Especially if you have *alot* of features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19a2391",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df599d4b",
   "metadata": {},
   "source": [
    "**In conclusion**, PCA is a great way to reduce dimensions of your data. However, if your stated goal is to also be able to *interpret* the inner workings of your data - then PCA is actually very bad since the dimensions it identifies can be any arbitrary combinations of your original features, making it impossible to understand what the model you'll train later on these new dimensions actually will base their predictions on.\n",
    "\n",
    "**As a second point**, PCA is another one of those algorithms which are very sensitive to different scales of your features. Therefore, make sure to scale your features before you apply PCA - if you have large differences of scale in your features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2efad5",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044fe82d",
   "metadata": {},
   "source": [
    "## Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b5855e",
   "metadata": {},
   "source": [
    "Let's use PCA in practice for a slightly larger dataset. We'll use the student performance dataset we worked with last week.\n",
    "\n",
    "*Note* We'll not focus on EDA or any rigurous evaluation here. Rather, the task is just to showcase how PCA can be applied."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe1cdda2",
   "metadata": {},
   "source": [
    "**Task 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c445c8",
   "metadata": {},
   "source": [
    "Import the dataset, drop G1 & G2 and one-hot-encode categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894d4fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "student_df = pd.read_csv(\"../lab/student-por.csv\", delimiter=\";\")\n",
    "\n",
    "student_df.drop(columns=[\"G2\", \"G1\"], inplace=True)\n",
    "\n",
    "X, y = student_df.drop(columns=['G3']), student_df['G3']\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59c6b3f6",
   "metadata": {},
   "source": [
    "As mentioned, we're just going to do a very quick and dirty preperation of our data (super sloppy, not reccommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2904825d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only the string/categorical columns\n",
    "categorical_columns = X.select_dtypes(include=['object']).columns\n",
    "\n",
    "# One-hot encode the categorical columns\n",
    "X = pd.get_dummies(X, columns=categorical_columns, dtype=int)\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4f4dc43",
   "metadata": {},
   "outputs": [],
   "source": [
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1268a72",
   "metadata": {},
   "source": [
    "All features are approximately at the same scale also (except for maybe age, but the difference isn't that significant) so we'll omit feature scaling now aswell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38df6c5",
   "metadata": {},
   "source": [
    "**Task 2**\n",
    "\n",
    "GridSearch a RandomForest to find a good performing set of hyperparameters on this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c7f113",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cab3823",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "                \"n_estimators\": [10, 20, 50, 100], \n",
    "                \"max_depth\": [10, 20, 30],  \n",
    "                \"min_samples_split\": [2, 5, 10],\n",
    "                \"min_samples_leaf\": [1, 2, 4]\n",
    "             }\n",
    "\n",
    "rf_classifier = RandomForestRegressor(random_state=42)\n",
    "\n",
    "score = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf_classifier, \n",
    "    param_grid=param_grid,\n",
    "    cv=3, \n",
    "    n_jobs=-1, \n",
    "    verbose=2,  \n",
    "    scoring=score,\n",
    ")\n",
    "\n",
    "# Utför grid search över alla möjliga kombinationer av dina hyperparameters\n",
    "grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61fc8bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_score_ * -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049e0d6c",
   "metadata": {},
   "source": [
    "**Task 3**\n",
    "\n",
    "Ok, now let's try to reduce our feature space dimension by applying PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592d9e66",
   "metadata": {},
   "source": [
    "Assume we want to reduce to, say 50 dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c88dbf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_reduced = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba686153",
   "metadata": {},
   "source": [
    "Note that we now only have 50 dimensions in our feature space. \n",
    "\n",
    "That's great, but at the same time we've lost all common sense of what these columns actually represent!\n",
    "\n",
    "All we know is that they represent the 50 directions in which we have the most variance, in our feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60e2b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's do the same gridsearch as above, but using these 50 principle components as our new features\n",
    "\n",
    "grid_search = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1536d3db",
   "metadata": {},
   "source": [
    "First, notice that it takes significantly longer to train here because the Tree's have many more questions to ask! Why is this?\n",
    "\n",
    "Look at the features again, they have continous values now instead of discrete - making the number of possible questions expontantially more.\n",
    "\n",
    "Have this training time increase in consideration, especially when training trees. Other families of models don't have the same problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e80c71ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.best_score_ * -1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad9098e",
   "metadata": {},
   "source": [
    "We got comparable results as above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20137d20",
   "metadata": {},
   "source": [
    "**Task 5**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11f0d18",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_best_score = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c0441a",
   "metadata": {},
   "source": [
    "Now do a foor-loop, where-in you each time reduce your original number of principle components (dimensionality of the feature space) to\n",
    "\n",
    "50, 45, 40, 35, 30, 25, 15, 5, 2\n",
    "\n",
    "Do a gridsearch for each of those case.\n",
    "\n",
    "For each iteration of the loop, append the best score to grid_best_score list defined above  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a13922",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "03488f04",
   "metadata": {},
   "source": [
    "**Task 6**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a99670e",
   "metadata": {},
   "source": [
    "Plot the list grid_best_score (y-axis) against the number of principle components (x-axis) so that you can compare the results for the grid searches above.\n",
    "\n",
    "Does the resulting plot look similar to what you find during previous week's lab?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0138c993",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
