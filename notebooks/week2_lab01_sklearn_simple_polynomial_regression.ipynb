{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Load the diabetes dataset\n",
    "used_cars_df = pd.read_csv('../data/used_cars.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory data analysis (EDA) and data cleaning\n",
    "\n",
    "The target for our regression problem here is the column *price*. \n",
    "\n",
    "Each row represent the characteristics of a car, and the corresponding sales price of said car. We are free to choose which of the available features to fit a model to, and try to predict the target with.\n",
    "\n",
    "It's generally essential that we use our wits and domain expertise to pick and engineer good features for our model. Bad features will produce a bad model, with poor predictive power. In other word, a useless model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remove a redundant column**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove the first column which looks like a copy of the index column\n",
    "\n",
    "columns_to_keep = used_cars_df.columns[1:]\n",
    "print(columns_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_cars_df = used_cars_df[columns_to_keep]\n",
    "\n",
    "used_cars_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Keep only numerical columns, for now**\n",
    "\n",
    "Many machine learning models require that the input are all numerical (since you can't do mathematic operations with anything else), and it is therefore essential that (when using models with that requirement) make sure that the data satisfies that condition.\n",
    "\n",
    "Note that there are ways to transform any given column into numericals that we can work with, but let's hold on with that for now and only keep the features that already are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_cars_df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_cars_df = used_cars_df.select_dtypes(include=['int64', 'float64'])\n",
    "\n",
    "used_cars_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Deal with missing data**\n",
    "\n",
    "Let's try to find and mitigate missing data. Note that whether to remove data points is a very sensitive decision, and should be carefully considered. \n",
    "\n",
    "Augmenting and fixing the data is a better alternative, if the time to do so is available. \n",
    "\n",
    "All changes we do to the training data *will* affect our model's performance, either insignificantly or significantly - depending on the changes we've made, and to what extent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for null-data\n",
    "\n",
    "used_cars_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll opt for the lazy way out here, and remove the null data since it's affecting a very low amount of records (not a good argument btw). \n",
    "\n",
    "This is generally not a recommended approach though, it might be well worth fixing the data instead.\n",
    "\n",
    "**Question**: What consequences on the data does our decision to remove these records potentially have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_cars_df = used_cars_df.dropna().reset_index(drop=True)\n",
    "\n",
    "used_cars_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dealing with unreasonable data**\n",
    "\n",
    "Usually, we have to spend considerable time to just clean the data and get rid of crap that has nested it's way into it.\n",
    "\n",
    "Crap in data is very common in real life."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by trying to understand the price column a bit better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(used_cars_df['price(in lakhs)'], bins=50);\n",
    "plt.xlabel('price(in lakhs)');\n",
    "plt.ylabel('count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's strange, it looks like there are a few cars that are extremely expensive. This is not incorrect per se, but let's look deeper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_cars_df[used_cars_df['price(in lakhs)']<=12.5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so we have 3 records of cars that looks to suspicous.\n",
    "\n",
    "Since Ali has been in India, he knows that 1 lakh is a common indian measure that means one hundred thousand (indian rupees, in this case).\n",
    "\n",
    "70000 lakhs is therefore 70000 * 100.000 = 7000000000 (indian rupees).\n",
    "\n",
    "Converting this to Swedish currency we get 877 447 200 SEK. Not reasonably at all. \n",
    "\n",
    "Let's just remove these records for simplicity, and plot again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_cars_df = used_cars_df[used_cars_df['price(in lakhs)']<12.5].reset_index(drop=True)\n",
    "\n",
    "plt.hist(used_cars_df['price(in lakhs)'], bins=50);\n",
    "\n",
    "plt.xlabel('price(in lakhs)');\n",
    "plt.ylabel('count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah, now it looks much more realistic!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, great. Let's also take a look at kms_driven"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(used_cars_df['kms_driven'], bins=50, color='green');\n",
    "\n",
    "plt.xlabel('kms_driven');\n",
    "plt.ylabel('count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, this also looks a little suspicious. Perhaps?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_cars_df[used_cars_df['kms_driven']>150000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, so there are only 6 cars that have droven over 150.000 kms. Let's remove them, since they deviate in too much in values from our other values, and thus much deteroriate the models performance.\n",
    "\n",
    "We can of course do this if we'd like, but let's think for a moment before doing so. What limitations are we putting on our model by removing these records?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "used_cars_df = used_cars_df[used_cars_df['kms_driven']<150000].reset_index(drop=True)\n",
    "\n",
    "plt.hist(used_cars_df['kms_driven'], bins=50, color='green');\n",
    "\n",
    "plt.xlabel('kms_driven');\n",
    "plt.ylabel('count');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, looks like we have a good range "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**Bonus task**\n",
    "\n",
    "\n",
    "Try to do some analysis (perhaps plots and calculating simple metrics such as min, max, mean, std etc.) on each of these remaining columns. Is there something in particular you find interesting? \n",
    "\n",
    "Can we do something about it? If you find any notable outliers, remove them for now.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll begin our model fitting by limiting ourselves to a single feature.\n",
    "\n",
    "We'll try to predict car prices using km_driven as the sole feature, for now. \n",
    "\n",
    "Note that this is obviously very limiting, but we do it for pedagocical reasons in order to both get used to the sk-learn package, and to learn an important lesson...\n",
    "\n",
    "In other words, we'll now assume that we can model\n",
    "\n",
    "$$ price = w_1 \\cdot (kms\\ driven) + w_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = used_cars_df['kms_driven'].values, used_cars_df['price(in lakhs)'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X, y)\n",
    "plt.xlabel('kms_driven');\n",
    "plt.ylabel('price(in lakhs)');\n",
    "\n",
    "#plt.ylim()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=40) # set a random state, so we can reproduce our results\n",
    "\n",
    "print('Train set:')\n",
    "print('X:', len(X_train))\n",
    "print('y:', len(y_train), end='\\n\\n')\n",
    "\n",
    "print('Test set:')\n",
    "print('X:', len(X_test))\n",
    "print('y:', len(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_train, y_train, label = 'train')\n",
    "plt.scatter(X_test, y_test, label = 'test')\n",
    "\n",
    "plt.xlabel('kms_driven');\n",
    "plt.ylabel('price(in lakhs)');\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats\n",
    "\n",
    "stats.pearsonr(used_cars_df['kms_driven'], used_cars_df['price(in lakhs)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import a linear regression model and the MSE-metric from sklearn\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this initialized a linear regression model. It has not trained on anything yet\n",
    "\n",
    "linear_regression_model = LinearRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These models require a 2D-input, but our current data is 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can mitigate this using the .reshape method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(len(X_train), 1)\n",
    "y_train = y_train.reshape(len(y_train), 1)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can pass in a -1 in the .reshape method aswell, it then automatically tries to infer the dimension given your data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)\n",
    "\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression_model.fit(X_train, y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression_model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_hat = linear_regression_model.predict(X_train)\n",
    "y_test_hat = linear_regression_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beräkna loss på train set\n",
    "\n",
    "print(mean_squared_error(y_train, y_train_hat))\n",
    "\n",
    "print(np.sqrt(mean_squared_error(y_train, y_train_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beräkna loss på test set\n",
    "\n",
    "print(mean_squared_error(y_test, y_test_hat))\n",
    "\n",
    "print(np.sqrt(mean_squared_error(y_test, y_test_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_test, y_test)\n",
    "plt.scatter(X_test, y_test_hat)\n",
    "\n",
    "plt.xlabel('kms_driven');\n",
    "plt.ylabel('price(in lakhs)');\n",
    "plt.title('Test data performance analysis');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is the above result any good? Can you draw any conclusions?\n",
    "\n",
    "If we really insisted, for some reason, on fitting a straight line to our data - what would be the correct course of action?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Polynomial regression**\n",
    "\n",
    "If we for some reason believe that a single feature in itself is not good enough, and that we might need powers of that feature, we can try to fit a polynomial model instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# assume that we want to create features for polynomial of degree two\n",
    "\n",
    "poly_transform = PolynomialFeatures(degree=2, include_bias=False) # this initializes our transformer\n",
    "\n",
    "X_train_polynomial = poly_transform.fit_transform(X_train)\n",
    "\n",
    "X_test_polynomial = poly_transform.transform(X_test)           # not that we do NOT fit on the test set, only transform it. More on this later.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we now have created are powers of our feature km_driven.\n",
    "\n",
    "In other words, we now have two columns where each are\n",
    "\n",
    "$km\\ driven, (km\\  driven)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_polynomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use these as features for our linear regression model we imported earlier (which supports multiple features).\n",
    "\n",
    "This would allow to model\n",
    "\n",
    "$$ price = w_2 \\cdot (km\\ driven)^2 + w_1 \\cdot (km\\ driven) + w_0 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize and fit the model\n",
    "\n",
    "polynomial_regression_degree_2_model = LinearRegression()      # NOTERA ATT DEN HÄR KLARAR AV BÅDE EN ELLER FLERA FEATURES\n",
    "\n",
    "polynomial_regression_degree_2_model.fit(X_train_polynomial, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_regression_degree_2_model.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polynomial_regression_degree_2_model.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict and calculate loss\n",
    "\n",
    "y_train_hat = polynomial_regression_degree_2_model.predict(X_train_polynomial)\n",
    "y_test_hat = polynomial_regression_degree_2_model.predict(X_test_polynomial)\n",
    "\n",
    "print('Train MSE:', mean_squared_error(y_train, y_train_hat))\n",
    "print('Test MSE:', mean_squared_error(y_test, y_test_hat))\n",
    "\n",
    "print('Train RMSE:', np.sqrt(mean_squared_error(y_train, y_train_hat)))\n",
    "print('Test RMSE:', np.sqrt(mean_squared_error(y_test, y_test_hat)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_test, y_test)\n",
    "plt.scatter(X_test, y_test_hat)\n",
    "\n",
    "plt.xlabel('kms_driven');\n",
    "plt.ylabel('price(in lakhs)');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is that really better in any meaningul way? \n",
    "\n",
    "Hmm... Is there some sort of conclusion we can draw here?\n",
    "\n",
    "What should we do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challanges\n",
    "\n",
    "**Task 1**\n",
    "\n",
    "Clearly, trying to model car prices using km_driven alone atleast seems difficult with linear or polynomial models.\n",
    "\n",
    "But if you were forced to do it anyway, what should we do? Look at the plots above and see if you can come up with an idea.\n",
    "\n",
    "<details>\n",
    "  <summary>Answer</summary>\n",
    "  Predicting for both expensive and cheaper cars simultaneously with one model doesn't seem to be a good idea. We might get much better performance if we instead split those stratas and train a model on each seperately.\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 2**\n",
    "\n",
    "Create two datasets from used_cars_df, called used_expensive_cars_df and used_cheap_cars_df. Define cheap to be a car that costs 12.5 lakh or less.\n",
    "\n",
    "Train a linear model on the used_cheap_cars_df data and try to predict the sales price using only kms_driven.\n",
    "\n",
    "What do you end up with? Is it better than before? \n",
    "\n",
    "*Hint:* You can use pretty much everything we've done above!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 3**\n",
    "\n",
    "Instead of kms_driven, now try using one of the other available features to model price. Do you get better performance? Limit your analysis to cheap cars.\n",
    "\n",
    "Which of the feature seem to be the single best one at predicting car price?\n",
    "\n",
    "*Hint:* You might need to deal with some unreasonable data in the other features aswell.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 4**\n",
    "\n",
    "We have not learned how to work with non-numeric columns yet as features for ML-models. However, see if you still can try to analyze the original columns we removed. Look for outliers, faulty data and other irregularities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
