{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1fc1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524d9e76",
   "metadata": {},
   "source": [
    "**Objective and goal for this lab**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca903507",
   "metadata": {},
   "source": [
    "This lab is intended to give a quick user guide on how to fit an AdaBoostClassifier. Analogously, you can just as well fit an AdaBoostRegressor for suitable problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da50fda6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04dc3c8",
   "metadata": {},
   "source": [
    "**Import the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8e60fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df = pd.read_csv('../data/IRIS.csv')\n",
    "iris_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdb9c72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "\n",
    "df = px.data.iris()\n",
    "fig = px.scatter_3d(iris_df, x='sepal_length', y='petal_length', z='petal_width', color='species')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e322160",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df['species'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd0cf22",
   "metadata": {},
   "source": [
    "The target column *species* is categorical. We need to make it numerical by assigning each class a specific number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7d8fee",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_map = {'Iris-setosa':0, 'Iris-versicolor':1, 'Iris-virginica': 2}\n",
    "\n",
    "numerical_targets = [class_map[value] for value in iris_df['species']]\n",
    "\n",
    "iris_df['species'] = numerical_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab7e232",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris_df['species'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25848419",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = iris_df.drop(columns='species'), iris_df['species']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e0c6d8",
   "metadata": {},
   "source": [
    "No further pre-processing or feature engineering is needed for this simple dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c49077",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0c9606",
   "metadata": {},
   "source": [
    "**GridSearch with AdaBoost**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e51446f",
   "metadata": {},
   "source": [
    "AdaBoost is in general composed by a sequential series of weak learners. The reason is that each one of them, by itself, will underfit - and thus have high bias. But, by using the boosting method, each weak learner can train to become good at what the previous learner was bad at. We then use all the trained learners for prediction, just like we did for bagging (in Random Forest). This will in theory eliminate the high bias problem of each individual learner, and instead give us a strong ensamble classifier/regressor.\n",
    "\n",
    "The default weak learner when using AdaBoost is a decision tree with max depth of 1 (this is really a weak learner, right!).\n",
    "\n",
    "If you don't specify otherwise when initializing AdaBoost, the default weak learner is thus a decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6439f183",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "# define the model with default hyperparameters\n",
    "model = AdaBoostClassifier()\n",
    "\n",
    "# define the grid of values to search\n",
    "param_grid = {'n_estimators': [10, 50, 100, 500],                 # antalet sekventiella 'svaga' modeller att träna\n",
    "              'learning_rate': [0.001, 0.01, 0.1, 1.0]}           # avgör hur mycket varje fel i iterationen ska viktas\n",
    "\n",
    "# define the grid search procedure\n",
    "grid_search = GridSearchCV(estimator=model, \n",
    "                           param_grid=param_grid, \n",
    "                           n_jobs=-1, \n",
    "                           cv=3, \n",
    "                           scoring='accuracy')\n",
    "\n",
    "# execute the grid search\n",
    "grid_result = grid_search.fit(X, y)\n",
    "\n",
    "# summarize the best score and configuration\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "print('---'*25, end='\\n\\n')\n",
    "\n",
    "# summarize all scores that were evaluated\n",
    "mean_test_scores = grid_result.cv_results_['mean_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean_test_score, param in zip(mean_test_scores, params):\n",
    "    print('params:')\n",
    "    print(f\"{param}\")\n",
    "    print('mean accuracy:')\n",
    "    print(f'{round(mean_test_score,4)}')\n",
    "    print('---'*25, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0150f4f0",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533158b9",
   "metadata": {},
   "source": [
    "Above, we mentioned that AdaBoost concists of weak learners that we train sequentually, each one tries to become good at what the previous one was lacking. However, you can actually controll yourself what these weak learners should be. You can in practice choose anything to serve the role of the weak learner! In fact, it doesn't really have to be a weak learner, it can also be a strong learner!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fa1342",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\n",
    "\n",
    "\n",
    "# define the model with default hyperparameters\n",
    "model = AdaBoostClassifier(estimator=RandomForestClassifier())    # Note that we've here chosen RandomForestClassifier (with default hyperparameters) \n",
    "                                                                  # as the weak learner!  \n",
    "\n",
    "# define the grid of values to search\n",
    "param_grid = {'n_estimators': [10, 50, 100, 500],                 \n",
    "              'learning_rate': [0.001, 0.01, 0.1, 1.0]}           \n",
    "\n",
    "# define the grid search procedure\n",
    "grid_search = GridSearchCV(estimator=model, \n",
    "                           param_grid=param_grid, \n",
    "                           n_jobs=-1, \n",
    "                           cv=3, \n",
    "                           scoring='accuracy')\n",
    "\n",
    "# execute the grid search\n",
    "grid_result = grid_search.fit(X, y)\n",
    "\n",
    "# summarize the best score and configuration\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "print('---'*25, end='\\n\\n')\n",
    "\n",
    "# summarize all scores that were evaluated\n",
    "mean_test_scores = grid_result.cv_results_['mean_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean_test_score, param in zip(mean_test_scores, params):\n",
    "    print('params:')\n",
    "    print(f\"{param}\")\n",
    "    print('mean accuracy:')\n",
    "    print(f'{round(mean_test_score,4)}')\n",
    "    print('---'*25, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a1ca12",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951c92a5",
   "metadata": {},
   "source": [
    "## Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1f0eb2",
   "metadata": {},
   "source": [
    "**Task 1**\n",
    "\n",
    "Make sure to completely understand the whole process we've laid out above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ade375d",
   "metadata": {},
   "source": [
    "**Task 2**\n",
    "\n",
    "Read more about [AdaBoostClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostClassifier.html) and [AdaBoostRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.AdaBoostRegressor.html) in respective documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d598242",
   "metadata": {},
   "source": [
    "**Task 3**\n",
    "\n",
    "Redo the grid search, but now include a search for the best *estimator* - in other words, do a GridSearch where you also use different models as 'weak learners'. \n",
    "\n",
    "Experiment away!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d459e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search.cv_results_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca51113",
   "metadata": {},
   "source": [
    "**Task 3.5**\n",
    "\n",
    "We can actually do a hyperparametersearch for both adaboost and a given base estimator - at the same time!\n",
    "\n",
    "This is done by using the following code:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "701cbd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the AdaBoost classifier with RandomForest as base estimator\n",
    "\n",
    "adaboost = AdaBoostClassifier(estimator=RandomForestClassifier())\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100],\n",
    "    'learning_rate': [0.01, 0.1, 1],\n",
    "    'estimator__n_estimators': [10, 50, 100],\n",
    "    'estimator__max_depth': [3, 5, 7]\n",
    "}\n",
    "\n",
    "# Perform Grid Search with Cross Validation\n",
    "grid_search = GridSearchCV(adaboost, \n",
    "                           param_grid, \n",
    "                           cv=3, \n",
    "                           scoring='accuracy', \n",
    "                           n_jobs=-1)\n",
    "\n",
    "grid_result = grid_search.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0e8f0b",
   "metadata": {},
   "source": [
    "As we've done previously, we can print out all the grid search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "841ec016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize the best score and configuration\n",
    "print(f\"Best: {grid_result.best_score_} using {grid_result.best_params_}\")\n",
    "print('---'*25, end='\\n\\n')\n",
    "\n",
    "# summarize all scores that were evaluated\n",
    "mean_test_scores = grid_result.cv_results_['mean_test_score']\n",
    "params = grid_result.cv_results_['params']\n",
    "for mean_test_score, param in zip(mean_test_scores, params):\n",
    "    print('params:')\n",
    "    print(f\"{param}\")\n",
    "    print('mean accuracy:')\n",
    "    print(f'{round(mean_test_score,4)}')\n",
    "    print('---'*25, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d7fda04",
   "metadata": {},
   "source": [
    "**Task 4**\n",
    "\n",
    "Now, create your own (more complicated) dataset using make_blobs, as we have previously. Use the following:\n",
    "\n",
    "n_samples = 2000\n",
    "\n",
    "n_features = 3\n",
    "\n",
    "n_classes = 6\n",
    "\n",
    "Do a 3D-plot of the data to make sure you get a grasp of it. Make sure the color of each point indicate class belonging.\n",
    "\n",
    "When that's done, also pick a random state such that the classes are somewhat difficult to distinguish.\n",
    "\n",
    "Therafter, redo Task 3 on this new dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a901dc0c",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
