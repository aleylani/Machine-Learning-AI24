{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Linear & Polynomial Regression\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case\n",
    "\n",
    "We are going to help a company to optimize their advertisement strategy. They spend money on advertisement for different media channels: TV, radio and newspaper - and wants to know which channel is the most most effective.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "- suggest marketing plan to increase sales units\n",
    "- use linear regression to predict sales based on different spendings on different marketing channels\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Initial EDA - Exploratory Data Analysis\n",
    "\n",
    "The dataset for this lecture comes from ISLR - Introduction to Statistical Learning. The dataset used is [Advertising.csv](https://www.kaggle.com/ishaanv/ISLR-Auto)\n",
    "\n",
    "Units:\n",
    "\n",
    "- TV, radio, newspaper - thousands dollars\n",
    "- Sales - thousands units\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"../data/advertising.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Insights via plots**\n",
    "\n",
    " Let's do some scatterplots, one for each feature vs the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_features = df.shape[1] - 1\n",
    "\n",
    "fig, ax = plt.subplots(1, number_features, figsize=(8, 3), dpi=100)\n",
    "\n",
    "for i, feature in enumerate(df.columns[:-1]):\n",
    "    sns.scatterplot(data=df, x=feature, y=\"Sales\", ax=ax[i])\n",
    "    ax[i].set(xlabel=\"Spending\", title=f\"{feature} spendings\")\n",
    "\n",
    "fig.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot pairwise relationships in a df\n",
    "ax = sns.pairplot(df, corner=True, height=2)\n",
    "\n",
    "# set corner to True as upper right mirrors the corner, this saves computations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Linear regression\n",
    "\n",
    "Linear regression is a supervised learning algorithm, the predicted output is continuous. \n",
    "\n",
    "When we have one feature (also called predictor) variable it's called simple linear regression. A linear regression model with one feature variable looks like this:\n",
    "\n",
    "$f_{w,b}(x) = w \\cdot x + b$\n",
    "\n",
    "The output of this model for any given x, is called the prediction of the model for that x. \n",
    "\n",
    "We done the output of the model $\\hat{y}$. In other words, we have\n",
    "\n",
    "$\\hat{y}^{i} = w \\cdot x^{i} + b$,\n",
    "\n",
    "where $x$ is the predictor (feature) variable, $\\hat{y}$ is the prediction, $w$ is the slope and $b$ the intercept of our model. \n",
    "\n",
    "The parameters of our model are unknown and needs to be estimated using our training data points $(x^1, y^1), (x^2, y^2), \\ldots, (x^m,y^m)$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "Let's try to predict sales numbers using only one feature. Let's choose TV spending as the feature, since that seems to have a better linear relationship with the target (sales numbers) than the other features. We draw this conclusion using visual inspection of our scatterplots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = df[\"TV\"], df[\"Sales\"]\n",
    "\n",
    "# fits a polynomial of degree deg using least squares polynomial fit.\n",
    "\n",
    "w, b = np.polyfit(x_train, y_train, deg=1)  # returns coefficient with highest power first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Estimated model parameters, based on our training data')\n",
    "print(f'w : {w}')\n",
    "print(f'b : {b}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's define our linear model\n",
    "\n",
    "def linear_model(x, w, b):\n",
    "\n",
    "    y_hat = w*x + b\n",
    "    \n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beräkna nu ut våra predictions, för varje punkt i vår dataset\n",
    "\n",
    "y_hats = [linear_model(x, w, b) for x in x_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.figure(figsize=(5, 3), dpi=100), plt.axes()\n",
    "\n",
    "sns.scatterplot(data=df, x=\"TV\", y=\"Sales\")\n",
    "sns.lineplot(x=x_train, y=y_hats, color=\"red\")\n",
    "\n",
    "ax.set(\n",
    "    title=\"TV advertisement linear regression\",\n",
    "    xlabel=\"Thousands dollars\",\n",
    "    ylabel=\"Sales thousands units\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use regplot to simultaneously do a scatter plot, and a plot of the 'best' fitting line on this data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# regression line using seaborn regplot\n",
    "\n",
    "sns.regplot(x=x_train, y=y_train, order=1, ci=None);\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Polynomial regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now try polynomial regression instead. This is just another another regression algorithm which might be better suited to some data than a linear regression model.  \n",
    "\n",
    "Let's assume we want to fit a polynomial of degree two. If we also have one feature, the polyonomial regression model of degree two then  looks like this:\n",
    "\n",
    "$f_{\\bold{w},b}(x) = w_2 \\cdot x^2 + w_1 \\cdot x + b$.\n",
    "\n",
    "The prediction, for training simple $i$ is thus denoted\n",
    "\n",
    "$\\hat{y}^{i} = w_2 \\cdot (x^{{i}})^2 + w_1 \\cdot x^{i} + b$,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2, w1, b = np.polyfit(x_train, y_train, deg=2)  # returns coefficient with highest power first\n",
    "\n",
    "print('Estimated model parameters, based on our training data')\n",
    "print(f'w2 : {w2}')\n",
    "print(f'w1 : {w1}')\n",
    "print(f'b  : {b}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's now define our polynomial model (of degree two)\n",
    "\n",
    "def polynomial_model(x, w2, w1, b):\n",
    "\n",
    "    y_hat = w2*x**2 + w1*x + b\n",
    "    \n",
    "    return y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# beräkna våra predicted y-values\n",
    "\n",
    "y_hats = [polynomial_model(x, w2, w1, b) for x in x_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the resulting predictions with the ground truth\n",
    "\n",
    "fig, ax = plt.figure(figsize=(5, 3), dpi=100), plt.axes()\n",
    "\n",
    "sns.scatterplot(data=df, x=\"TV\", y=\"Sales\")\n",
    "sns.lineplot(x=x_train, y=y_hats, color=\"red\")\n",
    "\n",
    "ax.set(\n",
    "    title=\"TV advertisement linear regression\",\n",
    "    xlabel=\"Thousands dollars\",\n",
    "    ylabel=\"Sales thousands units\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It indeed, atleast looks, as if this is a better fitting model to our data! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Again, we could use regplot to instantly plot our data, and a fitted polynomial of desired order\n",
    "\n",
    "sns.regplot(x=x_train, y=y_train, order=2, ci=None); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Challenges\n",
    "\n",
    "**Task 1**\n",
    "\n",
    "Try creating polynomial regression of higher orders and fit them on our data. Then, plot the results as we've done above.\n",
    "\n",
    "Which order of polynomial seem to fit our data best? Try polynomials of orders 3, 4 & 5.\n",
    "\n",
    "Try to solve this task without using regplot.\n",
    "\n",
    "**Task 2**\n",
    "\n",
    "What happens if you fit a polynomial of order 20, 50 or 100?\n",
    "\n",
    "What are your thoughts on the results?\n",
    "\n",
    "Tip: use regplot for this. \n",
    "\n",
    "**Task 3**\n",
    "\n",
    "Evaluating the models - how well did we predict $\\bf{y}$ with $\\hat{\\bf{y}}$?\n",
    "\n",
    "To answer this question we use several **evaluation metrics** or **loss functions**: \n",
    "\n",
    "- Mean Absolute Error (MAE) - mean of error between $\\bf{y}$ and ${\\hat{\\bf{y}}}$. The unit is same as measured quantity.\n",
    "\n",
    "$$MAE = \\frac{1}{m}\\sum_{i=1}^m |y^i - \\hat{y}^i|$$\n",
    "\n",
    "- Mean Squared Error (MSE) - mean of squared errors between $\\bf{y}$ and ${\\hat{\\bf{y}}}$. It punishes large errors, and the units are in square units of the measured quantity\n",
    "\n",
    "$$MSE = \\frac{1}{m}\\sum_{i=1}^m (y^i - \\hat{y}^i)^2$$\n",
    "\n",
    "- Root Mean Squared Error (RMSE) - square root of MSE between $\\bf{y}$ and ${\\hat{\\bf{y}}}$. It punishes large errors, and the units are same as measured quantity, hence easier to interpret.\n",
    "\n",
    "$$RMSE = \\sqrt{\\frac{1}{m}\\sum_{i=1}^m (y^i - \\hat{y}^i)^2}$$\n",
    "\n",
    "\n",
    "**For each of these metrics:** \n",
    "\n",
    "- Implement a function that, given two lists (of predicted $\\hat{y}$ and true $y$) calculates them and returns a single number.\n",
    "- Then, calculate each metric for the predictions of our linear model and polynomial models of degree 2 and 5. Present the results.\n",
    "- Which model fits the data best according to these metrics?\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
