{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1fc1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859e89e9",
   "metadata": {},
   "source": [
    "**Multiclass Classification**\n",
    "\n",
    "In the binary classification setting, models like Logistic Regression, Decision Trees and Random Forests are all suitable to being trained to distinguish between two classes, often labeled as \"positive\" and \"negative\", or 1 and 0. This is achieved by constructing decision boundaries that partition data into regions corresponding to each class. However, many models extend naturally to the multi-class classification aswell. Of those we've learned so far, both Decision Trees and Random Forest does (but not Logistic Regression). \n",
    "\n",
    "In multiclass classification, the goal is to classify instances into one of multiple classes, where each instance can belong to only one class. The extension to multiclass is simple:\n",
    "\n",
    "At the terminal nodes of each decision tree, we use the training data to find the corresponding probability of the occurance of each class. A new unseen instance is then predicted to be the class which has the highest probability of occuring, according to the training data. For the multiclass setting, this works precisely the same, we just look for the highest probable class amongst several classes, instead of just two.\n",
    "\n",
    "Random Forest is just the combination of many trees, so it works exactly as above. We just now take the majority voted class from many tree's instead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524d9e76",
   "metadata": {},
   "source": [
    "**Objective and goal for this lab**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca903507",
   "metadata": {},
   "source": [
    "We will showcase how to conduct multiclass classifcation on some dummy data. \n",
    "\n",
    "Simultaneously, we will also an example on how to correctly work with train/validation/test splits in practice to evaluate a given model.\n",
    "\n",
    "*Note*: what we won't focus on here are details such as data EDA or data pre-processing / feature engineering. We will simply state at what stage they apply. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da50fda6",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04dc3c8",
   "metadata": {},
   "source": [
    "**Let's generate and plot some synthetic data to work with**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8e60fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "# Generate synthetic multiclass data with make_blobs\n",
    "n_samples = 2000\n",
    "n_features = 3\n",
    "n_classes = 4  # Number of classes\n",
    "random_state = 4\n",
    "\n",
    "X, y = make_blobs(n_samples=n_samples, n_features=n_features, centers=n_classes, random_state=random_state, cluster_std=2)\n",
    "\n",
    "data_dict = {'x': X[:,0], 'y': X[:,1], 'z': X[:,2], 'class': y}\n",
    "data_df = pd.DataFrame(data_dict)\n",
    "\n",
    "df = px.data.iris()\n",
    "fig = px.scatter_3d(data_df, x='x', y='y', z='z', color='class')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a1ca12",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f914c838",
   "metadata": {},
   "source": [
    "**EDA, Data pre-processing, preliminary feature engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57321d01",
   "metadata": {},
   "source": [
    "Once we have the data loaded, we dive deep into it. This is where. We can also do needed pre-processing and some informed feature engineering choices here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7294bdc5",
   "metadata": {},
   "source": [
    "**Split the data**\n",
    "\n",
    "Once the data is ready, we split it to a train and test split.\n",
    "\n",
    "*Note*: Follow closely how we use these two sets during the analysis below. We will **not** use the test split for anything else than a final assessement of our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cfaabe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data as per usual\n",
    "\n",
    "test_size = 0.2\n",
    "\n",
    "\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y, test_size=test_size, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f64e1bbc",
   "metadata": {},
   "source": [
    "We will be using X_train_val and y_train_val to find the best hyperparameters for our model using e.g., GridSearchVC."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd46e9b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53d7c79e",
   "metadata": {},
   "source": [
    "**Hyperparameter tune a Decision Tree (multiclass) classifier**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c844916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [None, 5, 10, 15],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "\n",
    "tree_classifier = DecisionTreeClassifier(random_state=42)\n",
    "\n",
    "# Create GridSearchCV instance\n",
    "grid_search = GridSearchCV(estimator=tree_classifier, \n",
    "                           param_grid=param_grid, \n",
    "                           cv=5, \n",
    "                           scoring='accuracy')\n",
    "\n",
    "# Perform GridSearchCV on the training data\n",
    "grid_search.fit(X_train_val, y_train_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa32325",
   "metadata": {},
   "source": [
    "Now we can print the results from the gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7fb68a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the hyperparameters and their corresponding accuracy\n",
    "results = grid_search.cv_results_\n",
    "\n",
    "# Print out hyperparameters and accuracy for all the fitted models\n",
    "for i in range(len(results['params'])):\n",
    "    print(\"Model:\", i+1)\n",
    "    print(\"Hyperparameters:\", results['params'][i])\n",
    "    print(\"Mean validation accuracy:\", np.round(results['mean_test_score'][i],4))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a992554",
   "metadata": {},
   "source": [
    "We can also straight up get the best performing model hyperparameters alongside its score, directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9e649d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best parameters and the best estimator\n",
    "best_params = grid_search.best_params_\n",
    "best_score = grid_search.best_score_\n",
    "\n",
    "print(\"Best Parameters:\", best_params)\n",
    "print(\"Best accuracy:\", np.round(best_score,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c174114",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c56be5",
   "metadata": {},
   "source": [
    "In this section, you can repeat the hyperparameter search above for *any* other suitable model. For example, RandomForest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36db29f3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918bb587",
   "metadata": {},
   "source": [
    "**Further model analysis**\n",
    "\n",
    "Now, finding optimal hyperparameters for a given model is great. But, we naturally want to do further analysis on said models - in order to really assess their performance. To do this, we abandon the gridsearch for now (since we've already found our hyperparameters) \n",
    "\n",
    "1. do a manual split of X_train_val to X_train and X_val\n",
    "2. retrain respective model, with the hyperparameters we've identifier, on X_train, y_train\n",
    "3. evaluate and assess performance on X_val, y_val\n",
    "\n",
    "**Note**: We do NOT involve X_test, y_test at this point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f0d4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_size = 0.33\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=validation_size, random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c56c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create another Decision Tree classifier, with the optimal hyperparameters found via grid search\n",
    "tree_clf = DecisionTreeClassifier(criterion = 'gini', \n",
    "                                  max_depth = 10, \n",
    "                                  min_samples_leaf = 1, \n",
    "                                  min_samples_split = 2)\n",
    "\n",
    "# Train the classifier on the training data\n",
    "tree_clf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the validation data\n",
    "y_pred = tree_clf.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124953bc",
   "metadata": {},
   "source": [
    "Once we have our predictions on the validation set, we can do conduct as much and as deep *error analysis* we want. We will speak more about error analysis in an upcoming lecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d2546b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y_true=y_val, y_pred=y_pred)\n",
    "\n",
    "cm_display = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = [0, 1, 2, 3])\n",
    "\n",
    "cm_display.plot()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d420e262",
   "metadata": {},
   "source": [
    "Above is the confusion matrix for the multiclass classification case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3157167",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(\"Accuracy:\", accuracy, end='\\n\\n')\n",
    "\n",
    "# Generate and print classification report\n",
    "report = classification_report(y_val, y_pred)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33aa9210",
   "metadata": {},
   "source": [
    "A classification_report provides relevant measures, for all classes. We've talked about precision and recall before. \n",
    "\n",
    "f1-score is a middle-ground/mean between  precision and recall, and you can read more about it [here](https://medium.com/@nirajan.acharya666/understanding-precision-recall-f1-score-and-support-in-machine-learning-evaluation-7ec935e8512e).\n",
    "\n",
    "The support is simply the amount of existing class instances. For example, in our validation set, we have 129 instances of class 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3b988b",
   "metadata": {},
   "source": [
    "Using the resulting analysis here, we might want to go back to step 1 and do some feature engineering."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c811cc92",
   "metadata": {},
   "source": [
    "**We will learn more about error analysis, and how to further interpret these results & solve them, in an upcoming lecture** "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a431c11d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e699fd0",
   "metadata": {},
   "source": [
    "**Final report**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3cca91a",
   "metadata": {},
   "source": [
    "Ok, assume now that you've done an exhaustive search for the best features and the best model + hyperparamaters for this problem. \n",
    "\n",
    "It's now time to do a final performance assessement, and get the numbers that you will report.\n",
    "\n",
    "This is where the test set finally comes in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67811df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# begin by training the model(s) with the best hyperparameters you've identified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29727fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf = DecisionTreeClassifier(criterion = 'gini', \n",
    "                                  max_depth = 10, \n",
    "                                  min_samples_leaf = 1, \n",
    "                                  min_samples_split = 2)\n",
    "\n",
    "# rf_clf = RandomForestClassifier(...)\n",
    "\n",
    "# any_other_clf = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22e8e6ba",
   "metadata": {},
   "source": [
    "Now we train those model(s) on the *whole* train and validation split (which we created in the very first step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27063100",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_clf.fit(X_train_val, y_train_val)\n",
    "\n",
    "#rf_clf.fit(X_train_val, y_train_val)\n",
    "\n",
    "#any_other_clf(X_train_val, y_train_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09080962",
   "metadata": {},
   "source": [
    "Predict & evaluate the models above on the **test set**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f949c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_tree_clf = tree_clf.predict(X_test)\n",
    "\n",
    "#rf_clf_y_pred = ...\n",
    "\n",
    "#any_other_clf_y_pred = .."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e20546",
   "metadata": {},
   "source": [
    "Having the predictions on the X_test, we can calculate all the measures that are required/asked for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d079b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_accuracy = accuracy_score(y_test, y_pred_tree_clf)\n",
    "print(\"Accuracy:\", accuracy, end='\\n\\n')\n",
    "\n",
    "tree_report = classification_report(y_test, y_pred_tree_clf)\n",
    "print(\"Classification Report:\\n\", tree_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51b9c22",
   "metadata": {},
   "source": [
    "**These** are the numbers you will finally report as the assessed performance. \n",
    "\n",
    "You fetch one set of these numbers for *each* of the models above. Here, we only have the numbers for the best decision tree.\n",
    "\n",
    "**IMPORTANT**: at this point, you can NOT go back and hypertune or change anything else. The evaluation on the test set should be the **last** thing you do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0409c8",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951c92a5",
   "metadata": {},
   "source": [
    "## Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1f0eb2",
   "metadata": {},
   "source": [
    "**Task 1**\n",
    "\n",
    "Make sure to completely understand the whole process we've laid out above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ade375d",
   "metadata": {},
   "source": [
    "**Task 2**\n",
    "\n",
    "Now apply this methodology to the Iris dataset, which can be found [here](https://www.kaggle.com/datasets/arshid/iris-flower-dataset?resource=download). It's a dataset suitable for multiclass classifcation.\n",
    "\n",
    "When working on it, don't skip the EDA, pre-procecessing and feature engineering steps.\n",
    "\n",
    "Find the best performing hyperparameters for Decision Tree, Random Forest and KNN, and make a performance report on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8169acd",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
