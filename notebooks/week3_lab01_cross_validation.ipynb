{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "\n",
    "Previously, we've seen why it's important to split our dataset into a train and a test split, in order to more fairly assess the usefullness of our model. We saw that the performance metrics (loss) on the train data is, in itself, a bad assessement of this and that good performance on the train set does not imply good performance on the test set.\n",
    "\n",
    "In any case, we've also seen now that even this data splitting strategy has its limits, since we sometime can get lucky (or unlucky, depending on how you look at it) with how this split occurs. If we get a particularly bad split, we might obtain really good performance on the test set on models that otherwise are severly overfit or underfit to our data.\n",
    "\n",
    "To bypass this, we'll be using **k-fold cross-validation**. \n",
    "\n",
    "This is a very simple generalisation of the train and test split strategy, where we just construct *k* seperate splits of our full dataset into *k* different train and test splits.\n",
    "\n",
    "The idea is then to fit our model (with the chosen features) on each of these *k* different train/test splits and average the performance over each. This newly obtained perfromance average is then usually a much better assessement of the model's (with the given features) true performance.\n",
    "\n",
    "Simply put, here are the steps for *k*-fold cross validation:\n",
    "\n",
    "    1. \n",
    "Construct *k* different train and test splits from our full data. \n",
    "    \n",
    "    2. \n",
    "Train our model, with the given features, on each of the *k* different train and test splits.\n",
    "    \n",
    "    3. \n",
    "Calculate the performance metric after training (could e.g., be MAE).\n",
    "    \n",
    "    4. \n",
    "Average the MAE over the *k* different training runs.\n",
    "\n",
    "Usual values for *k* is somwhere around 5 or 10. Chosing larger values for *k* provides better estimates, but brings about larger compute costs.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first import our dataset. We'll use the bike demand dataset from the previous lab.\n",
    "\n",
    "We'll skip all the data analysis, data cleaning and feature engineering here so that we can\n",
    "showcase cross-validation instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/bike_rental/day.csv')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop unwanted and forbidden columns (why are casual and registered forbidden here?)\n",
    "\n",
    "df = df.drop(columns=['instant', 'dteday', 'casual', 'registered'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate features and target\n",
    "\n",
    "X, y = df.drop(columns=['cnt']), df['cnt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excplicit implementation of k-fold cross-validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an explicit implementation of k-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize list to store MAE values\n",
    "mae_values = []\n",
    "\n",
    "# Define the number of folds\n",
    "num_folds = 10\n",
    "\n",
    "# Loop through each fold\n",
    "\n",
    "for fold in range(num_folds):\n",
    "    \n",
    "    # for each fold, create a random train/test split\n",
    "\n",
    "    # NOTE, there are different philosophies for how to chose the test size here. \n",
    "    \n",
    "    # 1. We can keep it constant throughout each fold or\n",
    "\n",
    "    # 2. As some like to do it, choose the test size to be 1/num_folds\n",
    "    # chosing num_folds = 5 then means a test size of 0.2 = 20%. \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/num_folds)\n",
    "    \n",
    "    # Initialize linear regression model\n",
    "    model = LinearRegression()\n",
    "    \n",
    "    # Train the model using X_train and y_train\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_test_hats = model.predict(X_test)\n",
    "    \n",
    "    # Calculate Root Mean Absolute Error\n",
    "    mae = mean_absolute_error(y_test, y_test_hats)\n",
    "    \n",
    "    # Append MSE to list\n",
    "    mae_values.append(mae)\n",
    "\n",
    "# Print MAE values for each fold\n",
    "for i, mae in enumerate(mae_values):\n",
    "    print(f\"Fold {i+1} MAE: {mae}\")\n",
    "\n",
    "print()\n",
    "print('Average MAE:', np.mean(mae_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make it a function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "def my_cross_validator(model, X, y, evaluation_function, num_folds, test_size):\n",
    "\n",
    "    loss_test_values = []\n",
    "    loss_train_values = []\n",
    "\n",
    "    for fold in range(num_folds):\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)   # notera att detta skapar en helt random uppdelning av train/test split\n",
    "                                                                                         # för varje loop - vilket är precis vad vi vill.\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_test_hats = model.predict(X_test)\n",
    "        y_train_hats = model.predict(X_train)\n",
    "\n",
    "        test_loss = evaluation_function(y_test, y_test_hats)\n",
    "        train_loss = evaluation_function(y_train, y_train_hats)\n",
    "\n",
    "        loss_test_values.append(test_loss)\n",
    "        loss_train_values.append(train_loss)\n",
    "\n",
    "    return loss_test_values, loss_train_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "num_folds = 5\n",
    "test_size = 0.2\n",
    "\n",
    "test_losses, train_losses = my_cross_validator(model, X, y, mean_absolute_error, num_folds, test_size)\n",
    "\n",
    "\n",
    "print('test:')\n",
    "print(test_losses, end='\\n\\n')\n",
    "print('test average:')\n",
    "print(np.mean(test_losses), end='\\n\\n')\n",
    "\n",
    "print('train:')\n",
    "print(train_losses, end='\\n\\n' )\n",
    "print('train average:')\n",
    "print(np.mean(train_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(y, bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Sklearn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, mean_absolute_error\n",
    "\n",
    "\n",
    "# Initialize linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Define the scoring function\n",
    "scoring = {'mae': make_scorer(mean_absolute_error),\n",
    "           'mse': make_scorer(mean_squared_error)}\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "num_folds = 5\n",
    "\n",
    "cv_results = cross_validate(model, X, y, cv=num_folds, scoring=scoring, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract MAE values for each fold\n",
    "mae_values = cv_results['test_mae']\n",
    "\n",
    "# Print MAE values for each fold\n",
    "for i, mae in enumerate(mae_values):\n",
    "    print(f\"Fold {i+1} MAE: {mae}\")\n",
    "\n",
    "print()\n",
    "print('Average MAE:', np.mean(cv_results['test_mae']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To learn more about sklearns own cross-validators, read the documentation**\n",
    "\n",
    "You can either use:\n",
    "\n",
    "1. The function **cross_validation**, which we used above\n",
    "Documentation [here.](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html)\n",
    "\n",
    "2. The function **cross_val_score**\n",
    "Documentation [here.](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**How to use this**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use cross validation to assess the perfromance of *every* model and feature(s) pair!\n",
    "\n",
    "For example, assume we have two models, each called model_1 and model_2, respectively.\n",
    "\n",
    "Assume also that we now have features $x_1, x_2, x_3, x_4$.\n",
    "\n",
    "We now want to find out which model and feature combination that produces the pest result on our data.\n",
    "\n",
    "For starters, say that we want to assess model_1 combined with features $x_1, x_2$ and $x_3$.\n",
    "\n",
    "We then do a full 5-fold cross-validation on this model with this features. In other words:\n",
    "\n",
    "Let X consist of $x_1, x_2, x_3$.\n",
    "\n",
    "Then, we simply\n",
    "\n",
    "cv_results = cross_validate(model_1, X, y, cv=5, scoring=scoring).\n",
    "\n",
    "and therafter calculate\n",
    "\n",
    "combination_average_performance = np.mean(cv_results['test_mae'])\n",
    "\n",
    "This resulting average is what we use to compare performance with any other model and feature combination!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challanges\n",
    "\n",
    "**Task**\n",
    "\n",
    "Try using cross-validation to assess performance on the bike demand dataset using a linear model combined with different sets of features.\n",
    "\n",
    "You can use whichever method to cross-valdiate we've shown above, as you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/bike_rental/day.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
