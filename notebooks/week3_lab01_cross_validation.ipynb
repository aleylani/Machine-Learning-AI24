{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-validation\n",
    "\n",
    "Previously, we've seen why it's important to split our dataset into a train and a test split, in order to more fairly assess the usefullness of our model. We saw that the performance metrics (loss) on the train data is, in itself, a bad assessement of this and that good performance on the train set does not imply good performance on the test set.\n",
    "\n",
    "In any case, we've also seen now that even this data splitting strategy has its limits, since we sometime can get lucky (or unlucky, depending on how you look at it) with how this split occurs. If we get a particularly bad split, we might obtain really good performance on the test set on models that otherwise are severly overfit or underfit to our data.\n",
    "\n",
    "To bypass this, we'll be using **k-fold cross-validation**. \n",
    "\n",
    "This is a very simple generalisation of the train and test split strategy, where we just construct *k* seperate splits of our full dataset into *k* different train and test splits.\n",
    "\n",
    "The idea is then to fit our model (with the chosen features) on each of these *k* different train/test splits and average the performance over each. This newly obtained perfromance average is then usually a much better assessement of the model's (with the given features) true performance.\n",
    "\n",
    "Simply put, here are the steps for *k*-fold cross validation:\n",
    "\n",
    "    1. \n",
    "Construct *k* different train and test splits from our full data. \n",
    "    \n",
    "    2. \n",
    "Train our model, with the given features, on each of the *k* different train and test splits.\n",
    "    \n",
    "    3. \n",
    "Calculate the performance metric after training (could e.g., be MAE).\n",
    "    \n",
    "    4. \n",
    "Average the MAE over the *k* different training runs.\n",
    "\n",
    "Usual values for *k* is somwhere around 5 or 10. Chosing larger values for *k* provides better estimates, but brings about larger compute costs.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Implementation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first import our dataset. We'll use the bike demand dataset from the previous lab.\n",
    "\n",
    "We'll skip all the data analysis, data cleaning and feature engineering here so that we can\n",
    "showcase cross-validation instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instant</th>\n",
       "      <th>dteday</th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2011-01-01</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.344167</td>\n",
       "      <td>0.363625</td>\n",
       "      <td>0.805833</td>\n",
       "      <td>0.160446</td>\n",
       "      <td>331</td>\n",
       "      <td>654</td>\n",
       "      <td>985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2011-01-02</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.363478</td>\n",
       "      <td>0.353739</td>\n",
       "      <td>0.696087</td>\n",
       "      <td>0.248539</td>\n",
       "      <td>131</td>\n",
       "      <td>670</td>\n",
       "      <td>801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2011-01-03</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.196364</td>\n",
       "      <td>0.189405</td>\n",
       "      <td>0.437273</td>\n",
       "      <td>0.248309</td>\n",
       "      <td>120</td>\n",
       "      <td>1229</td>\n",
       "      <td>1349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2011-01-04</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.212122</td>\n",
       "      <td>0.590435</td>\n",
       "      <td>0.160296</td>\n",
       "      <td>108</td>\n",
       "      <td>1454</td>\n",
       "      <td>1562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2011-01-05</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.226957</td>\n",
       "      <td>0.229270</td>\n",
       "      <td>0.436957</td>\n",
       "      <td>0.186900</td>\n",
       "      <td>82</td>\n",
       "      <td>1518</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>727</td>\n",
       "      <td>2012-12-27</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.254167</td>\n",
       "      <td>0.226642</td>\n",
       "      <td>0.652917</td>\n",
       "      <td>0.350133</td>\n",
       "      <td>247</td>\n",
       "      <td>1867</td>\n",
       "      <td>2114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>728</td>\n",
       "      <td>2012-12-28</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.253333</td>\n",
       "      <td>0.255046</td>\n",
       "      <td>0.590000</td>\n",
       "      <td>0.155471</td>\n",
       "      <td>644</td>\n",
       "      <td>2451</td>\n",
       "      <td>3095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>729</td>\n",
       "      <td>2012-12-29</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.253333</td>\n",
       "      <td>0.242400</td>\n",
       "      <td>0.752917</td>\n",
       "      <td>0.124383</td>\n",
       "      <td>159</td>\n",
       "      <td>1182</td>\n",
       "      <td>1341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>730</td>\n",
       "      <td>2012-12-30</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.255833</td>\n",
       "      <td>0.231700</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>0.350754</td>\n",
       "      <td>364</td>\n",
       "      <td>1432</td>\n",
       "      <td>1796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>731</td>\n",
       "      <td>2012-12-31</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.215833</td>\n",
       "      <td>0.223487</td>\n",
       "      <td>0.577500</td>\n",
       "      <td>0.154846</td>\n",
       "      <td>439</td>\n",
       "      <td>2290</td>\n",
       "      <td>2729</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>731 rows × 16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     instant      dteday  season  yr  mnth  holiday  weekday  workingday  \\\n",
       "0          1  2011-01-01       1   0     1        0        6           0   \n",
       "1          2  2011-01-02       1   0     1        0        0           0   \n",
       "2          3  2011-01-03       1   0     1        0        1           1   \n",
       "3          4  2011-01-04       1   0     1        0        2           1   \n",
       "4          5  2011-01-05       1   0     1        0        3           1   \n",
       "..       ...         ...     ...  ..   ...      ...      ...         ...   \n",
       "726      727  2012-12-27       1   1    12        0        4           1   \n",
       "727      728  2012-12-28       1   1    12        0        5           1   \n",
       "728      729  2012-12-29       1   1    12        0        6           0   \n",
       "729      730  2012-12-30       1   1    12        0        0           0   \n",
       "730      731  2012-12-31       1   1    12        0        1           1   \n",
       "\n",
       "     weathersit      temp     atemp       hum  windspeed  casual  registered  \\\n",
       "0             2  0.344167  0.363625  0.805833   0.160446     331         654   \n",
       "1             2  0.363478  0.353739  0.696087   0.248539     131         670   \n",
       "2             1  0.196364  0.189405  0.437273   0.248309     120        1229   \n",
       "3             1  0.200000  0.212122  0.590435   0.160296     108        1454   \n",
       "4             1  0.226957  0.229270  0.436957   0.186900      82        1518   \n",
       "..          ...       ...       ...       ...        ...     ...         ...   \n",
       "726           2  0.254167  0.226642  0.652917   0.350133     247        1867   \n",
       "727           2  0.253333  0.255046  0.590000   0.155471     644        2451   \n",
       "728           2  0.253333  0.242400  0.752917   0.124383     159        1182   \n",
       "729           1  0.255833  0.231700  0.483333   0.350754     364        1432   \n",
       "730           2  0.215833  0.223487  0.577500   0.154846     439        2290   \n",
       "\n",
       "      cnt  \n",
       "0     985  \n",
       "1     801  \n",
       "2    1349  \n",
       "3    1562  \n",
       "4    1600  \n",
       "..    ...  \n",
       "726  2114  \n",
       "727  3095  \n",
       "728  1341  \n",
       "729  1796  \n",
       "730  2729  \n",
       "\n",
       "[731 rows x 16 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/bike_rental/day.csv')\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>yr</th>\n",
       "      <th>mnth</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weathersit</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>hum</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>cnt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.344167</td>\n",
       "      <td>0.363625</td>\n",
       "      <td>0.805833</td>\n",
       "      <td>0.160446</td>\n",
       "      <td>985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.363478</td>\n",
       "      <td>0.353739</td>\n",
       "      <td>0.696087</td>\n",
       "      <td>0.248539</td>\n",
       "      <td>801</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.196364</td>\n",
       "      <td>0.189405</td>\n",
       "      <td>0.437273</td>\n",
       "      <td>0.248309</td>\n",
       "      <td>1349</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.212122</td>\n",
       "      <td>0.590435</td>\n",
       "      <td>0.160296</td>\n",
       "      <td>1562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.226957</td>\n",
       "      <td>0.229270</td>\n",
       "      <td>0.436957</td>\n",
       "      <td>0.186900</td>\n",
       "      <td>1600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>726</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.254167</td>\n",
       "      <td>0.226642</td>\n",
       "      <td>0.652917</td>\n",
       "      <td>0.350133</td>\n",
       "      <td>2114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.253333</td>\n",
       "      <td>0.255046</td>\n",
       "      <td>0.590000</td>\n",
       "      <td>0.155471</td>\n",
       "      <td>3095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>728</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.253333</td>\n",
       "      <td>0.242400</td>\n",
       "      <td>0.752917</td>\n",
       "      <td>0.124383</td>\n",
       "      <td>1341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>729</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.255833</td>\n",
       "      <td>0.231700</td>\n",
       "      <td>0.483333</td>\n",
       "      <td>0.350754</td>\n",
       "      <td>1796</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.215833</td>\n",
       "      <td>0.223487</td>\n",
       "      <td>0.577500</td>\n",
       "      <td>0.154846</td>\n",
       "      <td>2729</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>731 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     season  yr  mnth  holiday  weekday  workingday  weathersit      temp  \\\n",
       "0         1   0     1        0        6           0           2  0.344167   \n",
       "1         1   0     1        0        0           0           2  0.363478   \n",
       "2         1   0     1        0        1           1           1  0.196364   \n",
       "3         1   0     1        0        2           1           1  0.200000   \n",
       "4         1   0     1        0        3           1           1  0.226957   \n",
       "..      ...  ..   ...      ...      ...         ...         ...       ...   \n",
       "726       1   1    12        0        4           1           2  0.254167   \n",
       "727       1   1    12        0        5           1           2  0.253333   \n",
       "728       1   1    12        0        6           0           2  0.253333   \n",
       "729       1   1    12        0        0           0           1  0.255833   \n",
       "730       1   1    12        0        1           1           2  0.215833   \n",
       "\n",
       "        atemp       hum  windspeed   cnt  \n",
       "0    0.363625  0.805833   0.160446   985  \n",
       "1    0.353739  0.696087   0.248539   801  \n",
       "2    0.189405  0.437273   0.248309  1349  \n",
       "3    0.212122  0.590435   0.160296  1562  \n",
       "4    0.229270  0.436957   0.186900  1600  \n",
       "..        ...       ...        ...   ...  \n",
       "726  0.226642  0.652917   0.350133  2114  \n",
       "727  0.255046  0.590000   0.155471  3095  \n",
       "728  0.242400  0.752917   0.124383  1341  \n",
       "729  0.231700  0.483333   0.350754  1796  \n",
       "730  0.223487  0.577500   0.154846  2729  \n",
       "\n",
       "[731 rows x 12 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop unwanted and forbidden columns (why are casual and registered forbidden here?)\n",
    "\n",
    "df = df.drop(columns=['instant', 'dteday', 'casual', 'registered'])\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate features and target\n",
    "\n",
    "X, y = df.drop(columns=['cnt']), df['cnt']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excplicit implementation of k-fold cross-validation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an explicit implementation of k-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 MAE: 726.1884913007088\n",
      "Fold 2 MAE: 755.9416342651093\n",
      "Fold 3 MAE: 617.5140240397105\n",
      "Fold 4 MAE: 614.9930526540138\n",
      "Fold 5 MAE: 645.5787586472427\n",
      "Fold 6 MAE: 701.0094591034607\n",
      "Fold 7 MAE: 681.1446158638439\n",
      "Fold 8 MAE: 623.5625753769953\n",
      "Fold 9 MAE: 765.6950317352204\n",
      "Fold 10 MAE: 592.6210147170318\n",
      "\n",
      "Average MAE: 672.4248657703336\n"
     ]
    }
   ],
   "source": [
    "# Initialize list to store MAE values\n",
    "mae_values = []\n",
    "\n",
    "# Define the number of folds\n",
    "num_folds = 10\n",
    "\n",
    "# Loop through each fold\n",
    "\n",
    "for fold in range(num_folds):\n",
    "    \n",
    "    # for each fold, create a random train/test split\n",
    "\n",
    "    # NOTE, there are different philosophies for how to chose the test size here. \n",
    "    \n",
    "    # 1. We can keep it constant throughout each fold or\n",
    "\n",
    "    # 2. As some like to do it, choose the test size to be 1/num_folds\n",
    "    # chosing num_folds = 5 then means a test size of 0.2 = 20%. \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=1/num_folds)\n",
    "    \n",
    "    # Initialize linear regression model\n",
    "    model = LinearRegression()\n",
    "    \n",
    "    # Train the model using X_train and y_train\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_test_hats = model.predict(X_test)\n",
    "    \n",
    "    # Calculate Root Mean Absolute Error\n",
    "    mae = mean_absolute_error(y_test, y_test_hats)\n",
    "    \n",
    "    # Append MSE to list\n",
    "    mae_values.append(mae)\n",
    "\n",
    "# Print MAE values for each fold\n",
    "for i, mae in enumerate(mae_values):\n",
    "    print(f\"Fold {i+1} MAE: {mae}\")\n",
    "\n",
    "print()\n",
    "print('Average MAE:', np.mean(mae_values))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Make it a function**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "\n",
    "def my_cross_validator(model, X, y, evaluation_function, num_folds, test_size):\n",
    "\n",
    "    loss_test_values = []\n",
    "    loss_train_values = []\n",
    "\n",
    "    for fold in range(num_folds):\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)   # notera att detta skapar en helt random uppdelning av train/test split\n",
    "                                                                                         # för varje loop - vilket är precis vad vi vill.\n",
    "        model.fit(X_train, y_train)\n",
    "\n",
    "        y_test_hats = model.predict(X_test)\n",
    "        y_train_hats = model.predict(X_train)\n",
    "\n",
    "        test_loss = evaluation_function(y_test, y_test_hats)\n",
    "        train_loss = evaluation_function(y_train, y_train_hats)\n",
    "\n",
    "        loss_test_values.append(test_loss)\n",
    "        loss_train_values.append(train_loss)\n",
    "\n",
    "    return loss_test_values, loss_train_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test:\n",
      "[617.8965799366107, 743.1819303531199, 744.3425759308585, 678.5059437347613, 701.6146624954099]\n",
      "\n",
      "test average:\n",
      "697.108338490152\n",
      "\n",
      "train:\n",
      "[653.858706933728, 614.7172752429974, 630.3502526068175, 634.0679736681291, 631.1159876170777]\n",
      "\n",
      "train average:\n",
      "632.8220392137499\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegression()\n",
    "num_folds = 5\n",
    "test_size = 0.2\n",
    "\n",
    "test_losses, train_losses = my_cross_validator(model, X, y, mean_absolute_error, num_folds, test_size)\n",
    "\n",
    "\n",
    "print('test:')\n",
    "print(test_losses, end='\\n\\n')\n",
    "print('test average:')\n",
    "print(np.mean(test_losses), end='\\n\\n')\n",
    "\n",
    "print('train:')\n",
    "print(train_losses, end='\\n\\n' )\n",
    "print('train average:')\n",
    "print(np.mean(train_losses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(y, bins=30)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using Sklearn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, mean_absolute_error\n",
    "\n",
    "\n",
    "# Initialize linear regression model\n",
    "model = LinearRegression()\n",
    "\n",
    "# Define the scoring function\n",
    "scoring = {'mae': make_scorer(mean_absolute_error),\n",
    "           'mse': make_scorer(mean_squared_error)}\n",
    "\n",
    "# Perform 5-fold cross-validation\n",
    "num_folds = 5\n",
    "\n",
    "cv_results = cross_validate(model, X, y, cv=num_folds, scoring=scoring, return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 MAE: 570.5941983602264\n",
      "Fold 2 MAE: 801.7763581756275\n",
      "Fold 3 MAE: 731.5481543179582\n",
      "Fold 4 MAE: 908.8770556825788\n",
      "Fold 5 MAE: 865.4749609748496\n",
      "\n",
      "Average MAE: 775.6541455022482\n"
     ]
    }
   ],
   "source": [
    "# Extract MAE values for each fold\n",
    "mae_values = cv_results['test_mae']\n",
    "\n",
    "# Print MAE values for each fold\n",
    "for i, mae in enumerate(mae_values):\n",
    "    print(f\"Fold {i+1} MAE: {mae}\")\n",
    "\n",
    "print()\n",
    "print('Average MAE:', np.mean(cv_results['test_mae']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To learn more about sklearns own cross-validators, read the documentation**\n",
    "\n",
    "You can either use:\n",
    "\n",
    "1. The function **cross_validation**, which we used above\n",
    "Documentation [here.](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html)\n",
    "\n",
    "2. The function **cross_val_score**\n",
    "Documentation [here.](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**How to use this**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use cross validation to assess the perfromance of *every* model and feature(s) pair!\n",
    "\n",
    "For example, assume we have two models, each called model_1 and model_2, respectively.\n",
    "\n",
    "Assume also that we now have features $x_1, x_2, x_3, x_4$.\n",
    "\n",
    "We now want to find out which model and feature combination that produces the pest result on our data.\n",
    "\n",
    "For starters, say that we want to assess model_1 combined with features $x_1, x_2$ and $x_3$.\n",
    "\n",
    "We then do a full 5-fold cross-validation on this model with this features. In other words:\n",
    "\n",
    "Let X consist of $x_1, x_2, x_3$.\n",
    "\n",
    "Then, we simply\n",
    "\n",
    "cv_results = cross_validate(model_1, X, y, cv=5, scoring=scoring).\n",
    "\n",
    "and therafter calculate\n",
    "\n",
    "combination_average_performance = np.mean(cv_results['test_mae'])\n",
    "\n",
    "This resulting average is what we use to compare performance with any other model and feature combination!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Challanges\n",
    "\n",
    "**Task**\n",
    "\n",
    "Try using cross-validation to assess performance on the bike demand dataset using a linear model combined with different sets of features.\n",
    "\n",
    "You can use whichever method to cross-valdiate we've shown above, as you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/bike_rental/day.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
