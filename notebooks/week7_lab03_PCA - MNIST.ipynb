{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc1fc1f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd20b11",
   "metadata": {},
   "source": [
    "**Objective**\n",
    "\n",
    "For this lab, we're going to see if we can apply PCA on the MNIST dataset to reduce the feature space dimensionality, and still get good performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6003b63b",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89508134",
   "metadata": {},
   "source": [
    "We begin by loading the MNIST-dataset, just as we did in the MNIST-lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c2d4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "\n",
    "def load_mnist_images(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        _, num, rows, cols = struct.unpack(\">IIII\", f.read(16))\n",
    "        return np.fromfile(f, dtype=np.uint8).reshape(num, rows, cols)\n",
    "\n",
    "def load_mnist_labels(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        _, num = struct.unpack(\">II\", f.read(8))\n",
    "        return np.fromfile(f, dtype=np.uint8)\n",
    "\n",
    "# Load training data\n",
    "train_images = load_mnist_images(\"../data/mnist/train-images.idx3-ubyte\")\n",
    "train_labels = load_mnist_labels(\"../data/mnist/train-labels.idx1-ubyte\")\n",
    "\n",
    "# Load test data\n",
    "test_images = load_mnist_images(\"../data/mnist/t10k-images.idx3-ubyte\")\n",
    "test_labels = load_mnist_labels(\"../data/mnist/t10k-labels.idx1-ubyte\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841aeb7d",
   "metadata": {},
   "source": [
    "Let's plot an randome image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c51511",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_images = len(train_images)\n",
    "\n",
    "random_index = np.random.randint(0, num_images)\n",
    "\n",
    "plt.imshow(train_images[random_index], cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee23baf",
   "metadata": {},
   "source": [
    "Note that ALOT of the pixels are just completely black (grayscale value of 0), and don't actually add any relevant information at all.\n",
    "\n",
    "*Hyphotesis*: PCA should, while identifying the important pieces of information and reducing dimensionality, get rid of precisely the reduntant information provided by these pixels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a72492",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfeff988",
   "metadata": {},
   "source": [
    "Let's transform our data into dataframes, and scale them (important for PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97095bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images_flattened = [list(image.flatten()) for image in train_images]\n",
    "\n",
    "train_images_flattened_array = np.array([image.flatten() for image in train_images])\n",
    "\n",
    "X_train = pd.DataFrame(train_images_flattened_array)\n",
    "\n",
    "X_train = X_train / 255                                                       # divide by the maximum grayscale value\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db409b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.DataFrame(train_labels)\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27edcb68",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images_flattened = [list(image.flatten()) for image in test_images]\n",
    "\n",
    "test_images_flattened_array = np.array([image.flatten() for image in test_images])\n",
    "\n",
    "X_test = pd.DataFrame(test_images_flattened_array)\n",
    "\n",
    "X_test = X_test / 255\n",
    "\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3d0062",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = pd.DataFrame(test_labels)\n",
    "\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84287409",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7a25e2c",
   "metadata": {},
   "source": [
    "Let's now define our PCA. \n",
    "\n",
    "**Important** To avoid data leagake, we must fit our PCA to only the train data, we then transform both the train and test data with what's been fitted on the train data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e8f531",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec97982",
   "metadata": {},
   "source": [
    "Make a guess as to how many dimensions we should reduce our feature space to.\n",
    "\n",
    "*hint*: take a look at the plots above. How many pixels do you think are reduntant (useless)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7e24c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_principal_components = 700          # your guess here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1dac4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=number_of_principal_components)\n",
    "\n",
    "# fit our PCA to the train_val set\n",
    "pca_transformer = pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4efee13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform both train_val and test sets using the fitted transformer\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "\n",
    "# convert the reduced dimensions to dataframes\n",
    "X_train_reduced = pd.DataFrame(X_train_pca)\n",
    "X_test_reduced = pd.DataFrame(X_test_pca)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4662ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reduced"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608bddb1",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b83d34fa",
   "metadata": {},
   "source": [
    "Train a (for example) KNeighborsClassifier, using the newly optained principle components as features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ca04ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23de0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier()\n",
    "knn.fit(X_train_reduced, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156e8411",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addfca38",
   "metadata": {},
   "source": [
    "Evaluate the trained model on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da7ea0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_pred = knn.predict(X_test_reduced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca346706",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true=y_test, y_pred=knn_pred.reshape(-1,1))\n",
    "\n",
    "cm_display = ConfusionMatrixDisplay(confusion_matrix = cm, display_labels = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
    "\n",
    "cm_display.plot()\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93cd47a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, knn_pred)\n",
    "print(\"Accuracy:\", accuracy, end='\\n\\n')\n",
    "\n",
    "report = classification_report(y_test, knn_pred)\n",
    "print(\"Classification Report:\\n\", report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c300d77",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0d82b1a",
   "metadata": {},
   "source": [
    "## Challenges"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6f6841",
   "metadata": {},
   "source": [
    "**Task 1**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0b9ee9",
   "metadata": {},
   "source": [
    "Now you go ahead and try different values for the number of dimensions to reduce your feature space to. \n",
    "\n",
    "How low can you reduce the feature space dimensionality, and still get good performance?\n",
    "\n",
    "Plot accuracy as a function of the number of principle components. Does the result make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143ef520",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracies = []\n",
    "\n",
    "number_of_principal_components = [1,2,3,5,10,15,20,30,50,70,100,150,250,400,550,700]\n",
    "\n",
    "for number in number_of_principal_components:\n",
    "\n",
    "    pca = PCA(n_components=number)\n",
    "\n",
    "    # fit our PCA to the train_val set\n",
    "    pca_transformer = pca.fit(X_train)\n",
    "\n",
    "    # transform both train_val and test sets using the fitted transformer\n",
    "    X_train_pca = pca.transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "\n",
    "    # convert the reduced dimensions to dataframes\n",
    "    X_train_reduced = pd.DataFrame(X_train_pca)\n",
    "    X_test_reduced = pd.DataFrame(X_test_pca)\n",
    "\n",
    "    knn = KNeighborsClassifier()\n",
    "    knn.fit(X_train_reduced, y_train)\n",
    "\n",
    "    accuracy = accuracy_score(y_test, knn_pred)\n",
    "\n",
    "    knn_pred = knn.predict(X_test_reduced)\n",
    "\n",
    "    accuracies.append(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421c9811",
   "metadata": {},
   "outputs": [],
   "source": [
    "for pc, acc in zip(number_of_principal_components, accuracies):\n",
    "    print(f\"Number of Principal Components: {pc}, Accuracy: {acc}\")\n",
    "\n",
    "plt.scatter(number_of_principal_components[:], accuracies[:])\n",
    "plt.xlabel('Number of Principle Components')\n",
    "plt.ylabel('Total accuracy')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machinelearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
